{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, using standard PyTorch; then using Lighting:\n",
    "\n",
    "1. Using dummy (random) data, load, say, 8GB into pinned system RAM.\n",
    "2. Push into GPU memory at start of training loop.\n",
    "3. Train batch\n",
    "\n",
    "Time each step.  See how to make steps async.\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "4 GBytes copy from CPU memory to GPU memory:\n",
    "\n",
    "```\n",
    "  Unpinned: 3.6 s\n",
    "    Pinned: 0.3 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.17 s, sys: 1.63 s, total: 10.8 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tensor_cpu_unpinned = torch.randn(1000, 1000, 1000)  # 4 GBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.51 s, sys: 1.5 s, total: 4.01 s\n",
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tensor_cuda_from_unpinned = tensor_cpu_unpinned.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 833 ms, sys: 1.6 s, total: 2.43 s\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tensor_pinned = tensor_cpu_unpinned.pin_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1914)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_pinned[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1914)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cpu_unpinned[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cpu_unpinned[0, 0, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1914)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_pinned[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinned_numpy = tensor_pinned.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.19137391"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinned_numpy[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinned_numpy[0, 0, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_pinned[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.57 ms, sys: 4.93 ms, total: 7.49 ms\n",
      "Wall time: 6.24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tensor_cuda_from_pinned = tensor_pinned.cuda(non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a contiguous chunk keeps the pinned memory.\n",
    "tensor_pinned[:10].is_pinned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking randome chunk stops the pinning.\n",
    "tensor_pinned[[1, 5, 7, 10]].is_pinned()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next up:  Put this into a simple DataLoader; and then in a Lightning training loop (where the forward and train functions do nothing, so the only delay is loading memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic plan:\n",
    "\n",
    "* Dataset loads a \"big chunk\" of data into (unpinned) CPU memory.\n",
    "  - When we do this for real, this could be done in a separate process.\n",
    "* Dataset then yields batches (sampled from the \"big chunk\" in memory).\n",
    "* All DataLoader does is turn the numpy arrays into pinned tensors.  It doesn't do any sampling.  It gets a pre-made batch from Dataset.\n",
    "* The training loop in Lightning then asynchonously loads that pinned data into GPU memory while the GPU is training.  The batch needs to be pinned to enable async copies into GPU RAM.\n",
    "\n",
    "Things that won't work:\n",
    "\n",
    "* I don't think we can use multiple processes to load data into the GPU or into pinned memory.  How would we share that memory between processes?\n",
    "* Pinning the \"big chunk\" probably isn't useful, because the sampled data isn't pinned **if we take random samples**.  But taking a contiguous chunk of pinned memory retains the pinning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "getattr(object, name[, default]) -> value\n",
       "\n",
       "Get a named attribute from an object; getattr(x, 'y') is equivalent to x.y.\n",
       "When a default argument is given, it is returned when the attribute doesn't\n",
       "exist; without it, an exception is raised in that case.\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "getattr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model, trainer, dataloader, dataset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, batch_size=64, n_loads_from_disk=1, n_samples_from_mem=128):\n",
    "        self.rng = np.random.default_rng()\n",
    "        self.batch_size = batch_size\n",
    "        self.n_loads_from_disk = n_loads_from_disk\n",
    "        self.n_samples_from_mem = n_samples_from_mem\n",
    "        \n",
    "    def __iter__(self):\n",
    "        print('start iter')\n",
    "        # Fake loading lots of data from disk -\n",
    "        # in reality this would be done in a separate process, which\n",
    "        # outputs unpinned data in CPU RAM.\n",
    "        data = self.rng.random(size=(1000, 256, 256), dtype=np.float32)\n",
    "        \n",
    "        # Pre-allocating pinned memory speeds things up a little.\n",
    "        #pinned = torch.empty(1000, 256, 256, dtype=torch.float32, pin_memory=True)\n",
    "        cuda_mem = torch.empty(1000, 256, 256, dtype=torch.float32, device='cuda')\n",
    "        n = len(data)\n",
    "        max_start_i = n - self.batch_size\n",
    "        for _ in range(self.n_loads_from_disk):\n",
    "            #data = torch.tensor(data, device='cuda')\n",
    "            #pinned = torch.from_numpy(data).pin_memory()\n",
    "            #pinned[:, :, :] = data\n",
    "            #cuda_mem[:, :, :] = torch.from_numpy(data).pin_memory()\n",
    "            cuda_mem.copy_(torch.from_numpy(data).pin_memory())\n",
    "            #cuda_mem.copy_(data)\n",
    "            for _ in range(self.n_samples_from_mem):\n",
    "                idx = torch.randint(high=n, size=(self.batch_size, ))\n",
    "                yield cuda_mem[idx]\n",
    "                #yield self.rng.choice(data, self.batch_size)\n",
    "                #start_i = self.rng.integers(0, max_start_i)\n",
    "                #end_i = start_i + self.batch_size\n",
    "                #yield pinned[start_i:end_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyIterableDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 103 µs, sys: 11 µs, total: 114 µs\n",
      "Wall time: 117 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=lambda x: x[0],\n",
    "    #collate_fn=lambda x: torch.tensor(x[0], device='cuda'),\n",
    "    #collate_fn=lambda x: torch.tensor(x[0]),\n",
    "    #pin_memory=True,\n",
    "    #num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start iter\n",
      "torch.Size([64, 256, 256])\n",
      "False\n",
      "cuda:0\n",
      "torch.float32\n",
      "torch.Size([64, 256, 256])\n",
      "False\n",
      "cuda:0\n",
      "torch.float32\n",
      "torch.Size([64, 256, 256])\n",
      "False\n",
      "cuda:0\n",
      "torch.float32\n",
      "torch.Size([64, 256, 256])\n",
      "False\n",
      "cuda:0\n",
      "torch.float32\n",
      "torch.Size([64, 256, 256])\n",
      "False\n",
      "cuda:0\n",
      "torch.float32\n",
      "CPU times: user 417 ms, sys: 36.6 ms, total: 454 ms\n",
      "Wall time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "for d in dataloader:\n",
    "    print(d.shape)\n",
    "    print(d.is_pinned())\n",
    "    print(d.device)\n",
    "    print(d.dtype)\n",
    "    if i > 3:\n",
    "        break\n",
    "    #d.cuda()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* copying numpy array to CUDA Tensor within DataLoader = 8.48s (but this won't allow us to async load into GPU)\n",
    "* Pinning batch by batch (pinning done by DataLoader) = 8.63s\n",
    "* No pinning = 8.71s\n",
    "* Pinning the \"big chunk\", and then taking contiguous slices: 9.64s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.6'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(256 * 256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256 * 256)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitAutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 4 M   \n",
      "1 | decoder | Sequential | 4 M   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: : 128it [00:01, 89.13it/s, loss=0.083, v_num=29] \n",
      "CPU times: user 56.6 s, sys: 6.7 s, total: 1min 3s\n",
      "Wall time: 42.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load 'big chunk' into GPU in pre-allocated GPU memory: 42.2s - PROBABLY THE RIGHT SOLUTION.  IT'S FAST.  AND IT PROVIDES LOTS OF FLEXIBILITY IN TERMS OF SAMPLING EACH BATCH.  THE CODE IS A TINY BIT MORE COMPLEX, BUT NOT BY MUCH!\n",
    "* 'big chunk' in pinned memory at start, then copied into pre-allocated GPU memory: 42.4s\n",
    "* load 'big chunk' into GPU in one go: 43.5s\n",
    "* 'big chunk' in pinned CPU memory, where we only allocate pinned memory once, and copy into the pinned memory: 45.8s\n",
    "* 'big chunk' in pinned CPU memory, using `from_numpy(data).pin_memory()`: 46.0s; 45.7s  CLEAN CODE, BUT VERY LIMITING IN TERMS OF CONSTRUCTING EACH BATCH\n",
    "* 'big chunk' in pinned CPU memory, where we only allocate pinned memory once, and copy into the pinned memory using `from_numpy`: 46.2s\n",
    "* 'big chunk' in pinned CPU memory, then individual contiguous chunks are taken: 47.2s or 47.3\n",
    "  - is a little slower, but has the advantage that we have to do less GPU memory management, and can run on GPUs with small amounts of RAM, and it's more like standard PyTorch code\n",
    "* 'big chunk' in unpinned CPU memory, random chunks are put into CUDA memory in the DataLoader: 1min 2s\n",
    "* 'big chunk' in unpinned CPU memory, and then individual random chunks are pinned in DataLoader: 1min 5s; 1min 2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* num_workers = 8:  wall time = 1min 9s\n",
    "* num_workers = 1:  wall time = 26.1s\n",
    "* load 'big chunk' into GPU in one go: 27.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_pv_yield_2",
   "language": "python",
   "name": "predict_pv_yield_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
