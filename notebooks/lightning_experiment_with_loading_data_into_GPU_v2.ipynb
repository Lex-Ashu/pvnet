{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, using standard PyTorch; then using Lighting:\n",
    "\n",
    "1. Using dummy (random) data, load, say, 8GB into pinned system RAM.\n",
    "2. Push into GPU memory at start of training loop.\n",
    "3. Train batch\n",
    "\n",
    "Time each step.  See how to make steps async.\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "4 GBytes copy from CPU memory to GPU memory:\n",
    "\n",
    "```\n",
    "  Unpinned: 3.6 s\n",
    "    Pinned: 0.3 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.34 s, sys: 1.67 s, total: 11 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tensor_cpu_unpinned = torch.randn(1000, 1000, 1000)  # 4 GBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.51 s, sys: 1.5 s, total: 4.01 s\n",
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tensor_cuda_from_unpinned = tensor_cpu_unpinned.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.22 s, sys: 2.89 s, total: 5.1 s\n",
      "Wall time: 4.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tensor_pinned = tensor_cpu_unpinned.pin_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.57 ms, sys: 4.93 ms, total: 7.49 ms\n",
      "Wall time: 6.24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tensor_cuda_from_pinned = tensor_pinned.cuda(non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a contiguous chunk keeps the pinned memory.\n",
    "tensor_pinned[:10].is_pinned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking randome chunk stops the pinning.\n",
    "tensor_pinned[[1, 5, 7, 10]].is_pinned()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next up:  Put this into a simple DataLoader; and then in a Lightning training loop (where the forward and train functions do nothing, so the only delay is loading memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic plan:\n",
    "\n",
    "* Dataset loads a \"big chunk\" of data into (unpinned) CPU memory.\n",
    "  - When we do this for real, this could be done in a separate process.\n",
    "* Dataset then yields batches (sampled from the \"big chunk\" in memory).\n",
    "* All DataLoader does is turn the numpy arrays into pinned tensors.  It doesn't do any sampling.  It gets a pre-made batch from Dataset.\n",
    "* The training loop in Lightning then asynchonously loads that pinned data into GPU memory while the GPU is training.  The batch needs to be pinned to enable async copies into GPU RAM.\n",
    "\n",
    "Things that won't work:\n",
    "\n",
    "* I don't think we can use multiple processes to load data into the GPU or into pinned memory.  How would we share that memory between processes?\n",
    "* Pinning the \"big chunk\" probably isn't useful, because the sampled data isn't pinned **if we take random samples**.  But taking a contiguous chunk of pinned memory retains the pinning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) -> Tensor\n",
       "\n",
       "Constructs a tensor with :attr:`data`.\n",
       "\n",
       ".. warning::\n",
       "\n",
       "    :func:`torch.tensor` always copies :attr:`data`. If you have a Tensor\n",
       "    ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_`\n",
       "    or :func:`torch.Tensor.detach`.\n",
       "    If you have a NumPy ``ndarray`` and want to avoid a copy, use\n",
       "    :func:`torch.as_tensor`.\n",
       "\n",
       ".. warning::\n",
       "\n",
       "    When data is a tensor `x`, :func:`torch.tensor` reads out 'the data' from whatever it is passed,\n",
       "    and constructs a leaf variable. Therefore ``torch.tensor(x)`` is equivalent to ``x.clone().detach()``\n",
       "    and ``torch.tensor(x, requires_grad=True)`` is equivalent to ``x.clone().detach().requires_grad_(True)``.\n",
       "    The equivalents using ``clone()`` and ``detach()`` are recommended.\n",
       "\n",
       "Args:\n",
       "    data (array_like): Initial data for the tensor. Can be a list, tuple,\n",
       "        NumPy ``ndarray``, scalar, and other types.\n",
       "\n",
       "Keyword args:\n",
       "    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
       "        Default: if ``None``, infers data type from :attr:`data`.\n",
       "    device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
       "        Default: if ``None``, uses the current device for the default tensor type\n",
       "        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
       "        for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
       "    requires_grad (bool, optional): If autograd should record operations on the\n",
       "        returned tensor. Default: ``False``.\n",
       "    pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
       "        the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
       "\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
       "    tensor([[ 0.1000,  1.2000],\n",
       "            [ 2.2000,  3.1000],\n",
       "            [ 4.9000,  5.2000]])\n",
       "\n",
       "    >>> torch.tensor([0, 1])  # Type inference on data\n",
       "    tensor([ 0,  1])\n",
       "\n",
       "    >>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\n",
       "                     dtype=torch.float64,\n",
       "                     device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor\n",
       "    tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n",
       "\n",
       "    >>> torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)\n",
       "    tensor(3.1416)\n",
       "\n",
       "    >>> torch.tensor([])  # Create an empty tensor (of size (0,))\n",
       "    tensor([])\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, batch_size=64, n_loads_from_disk=1, n_samples_from_mem=128):\n",
    "        self.rng = np.random.default_rng()\n",
    "        self.batch_size = batch_size\n",
    "        self.n_loads_from_disk = n_loads_from_disk\n",
    "        self.n_samples_from_mem = n_samples_from_mem\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Fake loading lots of data from disk -\n",
    "        # in reality this would be done in a separate process, which\n",
    "        # outputs unpinned data in CPU RAM.\n",
    "        data = self.rng.random(size=(1000, 256, 256), dtype=np.float32)\n",
    "        \n",
    "        # Pre-allocating pinned memory speeds things up a little.\n",
    "        #pinned = torch.empty(1000, 256, 256, dtype=torch.float32, pin_memory=True)\n",
    "        n = len(data)\n",
    "        max_start_i = n - self.batch_size\n",
    "        for _ in range(self.n_loads_from_disk):\n",
    "            #data = torch.tensor(data, device='cuda')\n",
    "            pinned = torch.from_numpy(data).pin_memory()\n",
    "            #pinned[:, :, :] = data\n",
    "            for _ in range(self.n_samples_from_mem):\n",
    "                #idx = torch.randint(high=n, size=(self.batch_size, ))\n",
    "                #yield data[idx]\n",
    "                #yield self.rng.choice(data, self.batch_size)\n",
    "                start_i = self.rng.integers(0, max_start_i)\n",
    "                end_i = start_i + self.batch_size\n",
    "                yield pinned[start_i:end_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyIterableDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 73 µs, sys: 12 µs, total: 85 µs\n",
      "Wall time: 87.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=lambda x: x[0],\n",
    "    #collate_fn=lambda x: torch.tensor(x[0], device='cuda'),\n",
    "    #collate_fn=lambda x: torch.tensor(x[0]),\n",
    "    #pin_memory=True,\n",
    "    #num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 256])\n",
      "True\n",
      "cpu\n",
      "torch.float32\n",
      "CPU times: user 331 ms, sys: 36.9 ms, total: 368 ms\n",
      "Wall time: 326 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for d in dataloader:\n",
    "    print(d.shape)\n",
    "    print(d.is_pinned())\n",
    "    print(d.device)\n",
    "    print(d.dtype)\n",
    "    break\n",
    "    #d.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* copying numpy array to CUDA Tensor within DataLoader = 8.48s (but this won't allow us to async load into GPU)\n",
    "* Pinning batch by batch (pinning done by DataLoader) = 8.63s\n",
    "* No pinning = 8.71s\n",
    "* Pinning the \"big chunk\", and then taking contiguous slices: 9.64s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.5'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(256 * 256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256 * 256)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitAutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 4 M   \n",
      "1 | decoder | Sequential | 4 M   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: : 128it [00:01, 84.00it/s, loss=0.083, v_num=21]\n",
      "CPU times: user 1min 1s, sys: 5.39 s, total: 1min 6s\n",
      "Wall time: 46 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load 'big chunk' into GPU in one go: 43.5s\n",
    "* 'big chunk' in pinned CPU memory, where we only allocate pinned memory once, and copy into the pinned memory: 45.8s\n",
    "* 'big chunk' in pinned CPU memory, using `from_numpy(data).pin_memory()`: 46.0s  PROBABLY THE CLEANEST & BEST\n",
    "* 'big chunk' in pinned CPU memory, where we only allocate pinned memory once, and copy into the pinned memory using `from_numpy`: 46.2s\n",
    "* 'big chunk' in pinned CPU memory, then individual contiguous chunks are taken: 47.2s or 47.3\n",
    "  - is a little slower, but has the advantage that we have to do less GPU memory management, and can run on GPUs with small amounts of RAM, and it's more like standard PyTorch code\n",
    "* 'big chunk' in unpinned CPU memory, random chunks are put into CUDA memory in the DataLoader: 1min 2s\n",
    "* 'big chunk' in unpinned CPU memory, and then individual random chunks are pinned in DataLoader: 1min 5s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* num_workers = 8:  wall time = 1min 9s\n",
    "* num_workers = 1:  wall time = 26.1s\n",
    "* load 'big chunk' into GPU in one go: 27.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_pv_yield_2",
   "language": "python",
   "name": "predict_pv_yield_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
