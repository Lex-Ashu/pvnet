{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "married-projection",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Our ultimate aim is to predict solar electricity power generation over the next few hours.\n",
    "\n",
    "## Loading terabytes of data efficiently from cloud storage\n",
    "\n",
    "We have several TB of satellite data. To keep the GPU fed with data during training, we need to read chunks of data quickly from the Zarr store; and we also want to load data asynchronously.  That is, while the GPU is training on the current batch, the data loader should simultaneously load the _next_ batch from disk.\n",
    "\n",
    "PyTorch makes this easy!  PyTorch's `DataLoader` spawns multiple worker processes when constructed with `num_workers` set to more than 1.  Each worker process receives a copy of the `SatelliteDataset` object.\n",
    "\n",
    "There is a small challenge: The code hangs when it gets to `enumerate(dataloader)` if we open the `xarray.DataArray` in the main process and copy that opened `DataArray` to the child processes.  Our solution is to delay the creation of the `DataArray` until _after_ the worker processes have been created.  PyTorch makes this easy by allowing us to pass a `worker_init_fn` to `DataLoader`.  `worker_init_fn` is called on each worker process.  Our `worker_init_fn` just has one job: to call `SatelliteDataset.per_worker_init()` which, in turn, opens the `DataArray`.\n",
    "\n",
    "This approach achieves read speeds of 600 MB/s from Google Cloud Storage to a single GCP VM with 12 vCPUs (as measured by `nethogs`).\n",
    "\n",
    "We use `IterableDataset` instead of `Dataset` so `SatelliteDataset` can pre-load the next example from disk and then block (on the `yield`) waiting for PyTorch to read that data.  This allows the worker processes to be processing the next data samples while the main process is training the current batch on the GPU.\n",
    "\n",
    "We can't pin the memory in each worker process because pinned memory can't be shared across processes.  Instead we ask `DataLoader` to pin the collated batch so that pytorch-lightning can asynchronously load the next batch from pinned CPU memory into GPU memory.\n",
    "\n",
    "The satellite data is stored on disk as `int16`.  To speed up the movement of the satellite data between processes and from the CPU memory to the GPU memory, we keep the data as `int16` until the data gets to the GPU, where it is converted to `float32` and normalised.\n",
    "\n",
    "### Loading data from disk into memory in chunks\n",
    "\n",
    "Cloud storage buckets can't seek into files like 'proper' POSIX filesystems can.  So, even if we just want 1 byte from a 1 GB file, we have to load the entire 1 GB file from the bucket.\n",
    "\n",
    "Zarr is designed with this challenge in mind.  Zarr gets round the inability of cloud storage buckets to seek by chunking the data into lots of small files.  But, still, we have to load entire Zarr chunks at once, even if we only want a part of a chunk.  And, even though we can pull 600 MB/s from a cloud storage bucket, the reads from the storage bucket are still the rate-limiting-step.  (GPUs are very fast and have a voracious appetite for data!)\n",
    "\n",
    "To get the most out of each disk read, our worker processes load several contiguous chunks of Zarr data from disk into memory at once.  We then randomly sample from the in-memory data multiple times, before loading another set of chunks from disk into memory.   This trick increases training speed by about 10x.\n",
    "\n",
    "Each Zarr chunk is 36 timesteps long and contains the entire geographical extent.  Each timestep is about 5 minutes apart, so each Zarr chunk spans 1.5 hours, assuming the timesteps are contiguous (more on contiguous chunks later).\n",
    "\n",
    "### Loading only daylight data\n",
    "\n",
    "We're interested in forecasting solar power generation, so we don't care about nighttime data :)\n",
    "\n",
    "In the UK in summer, the sun rises first in the north east, and sets last in the north west (see [video of June 2019](https://www.youtube.com/watch?v=IOp-tj-IJpk&t=0s)).  In summer, the north gets more hours of sunshine per day.\n",
    "\n",
    "In the UK in winter, the sun rises first in the south east, and sets last in the south west (see [video of Jan 2019](https://www.youtube.com/watch?v=CJ4prUVa2nQ)).  In winter, the south gets more hours of sunshine per day.\n",
    "\n",
    "|                        | Summer | Winter |\n",
    "|           ---:         |  :---: |  :---: |\n",
    "| Sun rises first in     | N.E.   | S.E.   |\n",
    "| Sun sets last in       | N.W.   | S.W.   |\n",
    "| Most hours of sunlight | North  | South  |\n",
    "\n",
    "We always load a pre-defined number of Zarr chunks from disk every disk load (defined by `n_chunks_per_disk_load`).\n",
    "\n",
    "Before training, we select timesteps which have at least some sunlight.  We do this by computing the clearsky global horizontal irradiance (GHI) for the four corners of the satellite imagery, and for all the timesteps in the dataset.  We only use timesteps where the maximum global horizontal irradiance across all four corners is above some threshold.\n",
    "\n",
    "(The 'clearsky [solar irradiance](https://en.wikipedia.org/wiki/Solar_irradiance)' is the amount of sunlight we'd expect on a clear day at a specific time and location. The SI unit of irradiance is watt per square meter.  The 'global horizontal irradiance' is the total sunlight that would hit a horizontal surface on the surface of the Earth.  The GHI is the sum of the direct irradiance (sunlight which takes a direct path from the Sun to the Earth's surface) and the diffuse horizontal irradiance (the sunlight scattered from the atmosphere)).\n",
    "\n",
    "### Finding contiguous sequences\n",
    "\n",
    "Once we have a list of 'lit' timesteps, we then find contiguous sequences (timeseries without any gaps).  And we then compute a list of contiguous Zarr chunks that we'll load at once during training.\n",
    "\n",
    "### Loading data during training\n",
    "\n",
    "During training, each worker process randomly picks multiple contiguous Zarr chunk sequences from the list of contiguous sequences pre-computed before training started.  The worker loads that data into memory and then randomly samples many samples from that in-memory data before loading more data from disk.\n",
    "\n",
    "#### Ensuring each batch contains a random sample of the dataset\n",
    "\n",
    "When PyTorch's `DataLoader` constructs a batch, it reads from just one worker process.  (This is not how I had _assumed_ it would work:  I assumed PyTorch would construct each batch by randomly sampling from all workers.)  This is an issue because, for stochastic gradient descent to work correctly, each batch must contain random samples of the dataset.  So it's not sufficient for each worker to load just one contiguous Zarr chunk (because then each batch would be made up entirely of samples from roughly the same time of day).  So, instead, each worker process loads multiple contiguous Zarr sequences into memory.  This also means that each worker must load quite a lot of data from disk.  To avoid training pausing while a worker process loads more data from disk, the data loading is done asynchronously using a separate thread within each worker process.\n",
    "\n",
    "## Timestep numbering:\n",
    "\n",
    "* t<sub>0</sub> is 'now': the most recent observation.\n",
    "* t<sub>1</sub> is the first timestep of the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adequate-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python core\n",
    "from typing import Optional, Callable, TypedDict, Union, Iterable, Tuple, NamedTuple, List\n",
    "from dataclasses import dataclass\n",
    "import datetime\n",
    "from itertools import product\n",
    "from concurrent import futures\n",
    "import multiprocessing  # Just for setting start_method\n",
    "\n",
    "# Necessary, otherwise the code hangs when we try to use gcsfs from multiple processes!\n",
    "# See https://github.com/dask/gcsfs/issues/379\n",
    "# Must be called before PyTorch in imported.\n",
    "multiprocessing.set_start_method('spawn')\n",
    "\n",
    "# Scientific python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numcodecs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cloud compute\n",
    "import gcsfs\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# PV & geospatial\n",
    "import pvlib\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf9f0a88-1adb-49ef-903e-fdb5b9e4793a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1905e1e5-c3ad-4ba5-a937-a03ecbb5103f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021.04.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcsfs.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7896eb75-719b-4479-bd86-bc6b8e3e2352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbcfc691-e7d7-47b4-9def-a72c7e05b157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "605964ed-8f7c-4f35-a9b4-24f974dd24bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyproj.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-rainbow",
   "metadata": {},
   "source": [
    "## Consts & config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-humor",
   "metadata": {},
   "source": [
    "The [Zarr docs](https://zarr.readthedocs.io/en/stable/tutorial.html#configuring-blosc) say we should tell the Blosc compression library not to use threads because we're using multiple processes to read from our Zarr store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "complete-perry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numcodecs.blosc.use_threads = False\n",
    "\n",
    "ZARR = 'solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/OSGB36/all_zarr_int16'\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-quarterly",
   "metadata": {},
   "source": [
    "# Load satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fifteen-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sat_data(filename: str=ZARR) -> xr.DataArray:\n",
    "    \"\"\"Lazily opens the Zarr store on Google Cloud Storage (GCS).\n",
    "    \n",
    "    Selects the High Resolution Visible (HRV) satellite channel.\n",
    "    \"\"\"\n",
    "    print('get_sat_data(filename=', filename, ')')\n",
    "    gcs = gcsfs.GCSFileSystem() # access='read_only')\n",
    "    print('gcs')\n",
    "    store = gcsfs.GCSMap(root=filename, gcs=gcs)\n",
    "    print('store')\n",
    "    dataset = xr.open_zarr(store, consolidated=True)\n",
    "    print('dataset')\n",
    "    return dataset['stacked_eumetsat_data'].sel(variable='HRV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fe0be8d-de3e-41a2-a2bd-2ec7cb07d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sat_data.close(); del sat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "moderate-escape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_sat_data(filename= solar-pv-nowcasting-data/satellite/EUMETSAT/SEVIRI_RSS/OSGB36/all_zarr_int16 )\n",
      "gcs\n",
      "store\n",
      "dataset\n",
      "CPU times: user 1.69 s, sys: 81.3 ms, total: 1.77 s\n",
      "Wall time: 3.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sat_data = get_sat_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-draft",
   "metadata": {},
   "source": [
    "Caution: Wierdly, plotting `sat_data` at this point causes the code to hang (with no errors messages) when it gets to `for batch in dataloader:`.  The code hangs even if we first do `sat_data.close(); del sat_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "german-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-filing",
   "metadata": {},
   "source": [
    "Get the timestep indicies of each Zarr chunk.  Later, we will use these boundaries to ensure we load complete chunks at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "naked-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_chunk_boundaries = np.concatenate(([0], np.cumsum(sat_data.chunks[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-continent",
   "metadata": {},
   "source": [
    "## Select daylight hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "previous-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OSGB is also called \"OSGB 1936 / British National Grid -- United Kingdom Ordnance Survey\".\n",
    "# OSGB is used in many UK electricity system maps, and is used by the UK Met Office UKV model.\n",
    "# OSGB is a Transverse Mercator projection, using 'easting' and 'northing' coordinates\n",
    "# which are in meters.\n",
    "OSGB = 27700\n",
    "\n",
    "# WGS84 is short for \"World Geodetic System 1984\", used in GPS. Uses latitude and longitude.\n",
    "WGS84 = 4326\n",
    "\n",
    "# osgb_to_wgs84.transform() returns latitude (north-south), longitude (east-west)\n",
    "osgb_to_wgs84 = pyproj.Transformer.from_crs(crs_from=OSGB, crs_to=WGS84)\n",
    "\n",
    "\n",
    "def get_daylight_timestamps(\n",
    "    dt_index: pd.DatetimeIndex, \n",
    "    locations: Iterable[Tuple[float, float]],\n",
    "    ghi_threshold: float = 1\n",
    "    ) -> pd.DatetimeIndex:\n",
    "    \"\"\"Returns datetimes for which the global horizontal irradiance\n",
    "    (GHI) is above ghi_threshold across all locations.\n",
    "    \n",
    "    Args:\n",
    "      dt_index: DatetimeIndex to filter.  Must be UTC.\n",
    "      locations: List of Tuples of x, y coordinates in OSGB projection.\n",
    "      ghi_threshold: Global horizontal irradiance threshold.\n",
    "    \"\"\"\n",
    "    assert dt_index.tz.zone == 'UTC'\n",
    "    ghi_for_all_locations = []\n",
    "    for x, y in locations:\n",
    "        lat, lon = osgb_to_wgs84.transform(x, y)\n",
    "        location = pvlib.location.Location(latitude=lat, longitude=lon)\n",
    "        clearsky = location.get_clearsky(dt_index)\n",
    "        ghi = clearsky['ghi']\n",
    "        ghi_for_all_locations.append(ghi)\n",
    "        \n",
    "    ghi_for_all_locations = pd.concat(ghi_for_all_locations, axis='columns')\n",
    "    max_ghi = ghi_for_all_locations.max(axis='columns')\n",
    "    mask = max_ghi > ghi_threshold\n",
    "    return dt_index[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "champion-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEO_BORDER: int = 64  #: In same geo projection and units as sat_data.\n",
    "corners = [\n",
    "    (sat_data.x.values[x], sat_data.y.values[y]) \n",
    "    for x, y in product(\n",
    "        [GEO_BORDER, -GEO_BORDER], \n",
    "        [GEO_BORDER, -GEO_BORDER])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "significant-power",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.67 s, sys: 95.1 ms, total: 6.76 s\n",
      "Wall time: 6.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "datetimes = get_daylight_timestamps(\n",
    "    dt_index=pd.DatetimeIndex(sat_data.time.values, tz='UTC'), \n",
    "    locations=corners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d7871d1-327f-4b5a-aa8e-d53a1ace48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sat_data.close()\n",
    "#del sat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-effort",
   "metadata": {},
   "source": [
    "## Get contiguous segments of satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "incomplete-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segment(NamedTuple):\n",
    "    \"\"\"Represents the start and end indicies of a segment of contiguous samples.\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "\n",
    "\n",
    "def get_contiguous_segments(dt_index: pd.DatetimeIndex, min_timesteps: int, max_gap: pd.Timedelta) -> Iterable[Segment]:\n",
    "    \"\"\"Chunk datetime index into contiguous segments, each at least min_timesteps long.\n",
    "    \n",
    "    max_gap defines the threshold for what constitutes a 'gap' between contiguous segments.\n",
    "    \n",
    "    Throw away any timesteps in a sequence shorter than min_timesteps long.\n",
    "    \"\"\"\n",
    "    gap_mask = np.diff(dt_index) > max_gap\n",
    "    gap_indices = np.argwhere(gap_mask)[:, 0]\n",
    "\n",
    "    # gap_indicies are the indices into dt_index for the timestep immediately before the gap.\n",
    "    # e.g. if the datetimes at 12:00, 12:05, 18:00, 18:05 then gap_indicies will be [1].\n",
    "    segment_boundaries = gap_indices + 1\n",
    "\n",
    "    # Capture the last segment of dt_index.\n",
    "    segment_boundaries = np.concatenate((segment_boundaries, [len(dt_index)]))\n",
    "\n",
    "    segments = []\n",
    "    start_i = 0\n",
    "    for end_i in segment_boundaries:\n",
    "        n_timesteps = end_i - start_i\n",
    "        if n_timesteps >= min_timesteps:\n",
    "            segment = Segment(start=start_i, end=end_i)\n",
    "            segments.append(segment)\n",
    "        start_i = end_i\n",
    "        \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "defined-aberdeen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 781 ms, sys: 6.57 ms, total: 788 ms\n",
      "Wall time: 786 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Segment(start=0, end=222),\n",
       " Segment(start=222, end=445),\n",
       " Segment(start=445, end=669),\n",
       " Segment(start=669, end=893),\n",
       " Segment(start=893, end=1117)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "contiguous_segments = get_contiguous_segments(\n",
    "    dt_index = datetimes,\n",
    "    min_timesteps = 36 * 1.5,\n",
    "    max_gap = pd.Timedelta('5 minutes'))\n",
    "\n",
    "contiguous_segments[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "infrared-drama",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "598"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contiguous_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-diagram",
   "metadata": {},
   "source": [
    "## Turn the contiguous segments into sequences of Zarr chunks, which will be loaded together during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "celtic-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zarr_chunk_sequences(\n",
    "    n_chunks_per_disk_load: int, \n",
    "    zarr_chunk_boundaries: Iterable[int], \n",
    "    contiguous_segments: Iterable[Segment]) -> Iterable[Segment]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "      n_chunks_per_disk_load: Maximum number of Zarr chunks to load from disk in one go.\n",
    "      zarr_chunk_boundaries: The indicies into the Zarr store's time dimension which define the Zarr chunk boundaries.\n",
    "        Must be sorted.\n",
    "      contiguous_segments: Indicies into the Zarr store's time dimension that define contiguous timeseries.\n",
    "        That is, timeseries with no gaps.\n",
    "    \n",
    "    Returns zarr_chunk_sequences: a list of Segments representing the start and end indicies of contiguous sequences of multiple Zarr chunks,\n",
    "    all exactly n_chunks_per_disk_load long (for contiguous segments at least as long as n_chunks_per_disk_load zarr chunks),\n",
    "    and at least one side of the boundary will lie on a 'natural' Zarr chunk boundary.\n",
    "    \n",
    "    For example, say that n_chunks_per_disk_load = 3, and the Zarr chunks sizes are all 5:\n",
    "    \n",
    "    \n",
    "                  0    5   10   15   20   25   30   35 \n",
    "                  |....|....|....|....|....|....|....|\n",
    "\n",
    "    INPUTS:\n",
    "                     |------CONTIGUOUS SEGMENT----|\n",
    "                     \n",
    "    zarr_chunk_boundaries:\n",
    "                  |----|----|----|----|----|----|----|\n",
    "    \n",
    "    OUTPUT:\n",
    "    zarr_chunk_sequences:\n",
    "           3 to 15:  |-|----|----|\n",
    "           5 to 20:    |----|----|----|\n",
    "          10 to 25:         |----|----|----|\n",
    "          15 to 30:              |----|----|----|\n",
    "          20 to 32:                   |----|----|-|\n",
    "    \n",
    "    \"\"\"\n",
    "    assert n_chunks_per_disk_load > 0\n",
    "    \n",
    "    zarr_chunk_sequences = []\n",
    "\n",
    "    for contig_segment in contiguous_segments:\n",
    "        # searchsorted() returns the index into zarr_chunk_boundaries at which contig_segment.start\n",
    "        # should be inserted into zarr_chunk_boundaries to maintain a sorted list.\n",
    "        # i_of_first_zarr_chunk is the index to the element in zarr_chunk_boundaries which defines\n",
    "        # the start of the current contig chunk.\n",
    "        i_of_first_zarr_chunk = np.searchsorted(zarr_chunk_boundaries, contig_segment.start)\n",
    "        \n",
    "        # i_of_first_zarr_chunk will be too large by 1 unless contig_segment.start lies\n",
    "        # exactly on a Zarr chunk boundary.  Hence we must subtract 1, or else we'll\n",
    "        # end up with the first contig_chunk being 1 + n_chunks_per_disk_load chunks long.\n",
    "        if zarr_chunk_boundaries[i_of_first_zarr_chunk] > contig_segment.start:\n",
    "            i_of_first_zarr_chunk -= 1\n",
    "            \n",
    "        # Prepare for looping to create multiple Zarr chunk sequences for the current contig_segment.\n",
    "        zarr_chunk_seq_start_i = contig_segment.start\n",
    "        zarr_chunk_seq_end_i = None  # Just a convenience to allow us to break the while loop by checking if zarr_chunk_seq_end_i != contig_segment.end.\n",
    "        while zarr_chunk_seq_end_i != contig_segment.end:\n",
    "            zarr_chunk_seq_end_i = zarr_chunk_boundaries[i_of_first_zarr_chunk + n_chunks_per_disk_load]\n",
    "            zarr_chunk_seq_end_i = min(zarr_chunk_seq_end_i, contig_segment.end)\n",
    "            zarr_chunk_sequences.append(Segment(start=zarr_chunk_seq_start_i, end=zarr_chunk_seq_end_i))\n",
    "            i_of_first_zarr_chunk += 1\n",
    "            zarr_chunk_seq_start_i = zarr_chunk_boundaries[i_of_first_zarr_chunk]\n",
    "            \n",
    "    return zarr_chunk_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "funded-breathing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Segment(start=0, end=108),\n",
       " Segment(start=36, end=144),\n",
       " Segment(start=72, end=180),\n",
       " Segment(start=108, end=216),\n",
       " Segment(start=144, end=222),\n",
       " Segment(start=222, end=324),\n",
       " Segment(start=252, end=360),\n",
       " Segment(start=288, end=396),\n",
       " Segment(start=324, end=432),\n",
       " Segment(start=360, end=445)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zarr_chunk_sequences = get_zarr_chunk_sequences(\n",
    "    n_chunks_per_disk_load=3,\n",
    "    zarr_chunk_boundaries=zarr_chunk_boundaries,\n",
    "    contiguous_segments=contiguous_segments)\n",
    "\n",
    "zarr_chunk_sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-sponsorship",
   "metadata": {},
   "source": [
    "## PyTorch data storage & processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "falling-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "Array = Union[np.ndarray, xr.DataArray]\n",
    "\n",
    "IMAGE_ATTR_NAMES = ('historical_sat_images', 'target_sat_images')\n",
    "\n",
    "class Sample(TypedDict):\n",
    "    \"\"\"Simple class for structuring data for the ML model.\n",
    "    \n",
    "    Using typing.TypedDict gives us several advantages:\n",
    "      1. Single 'source of truth' for the type and documentation of each example.\n",
    "      2. A static type checker can check the types are correct.\n",
    "\n",
    "    Instead of TypedDict, we could use typing.NamedTuple,\n",
    "    which would provide runtime checks, but the deal-breaker with Tuples is that they're immutable\n",
    "    so we cannot change the values in the transforms.\n",
    "    \"\"\"\n",
    "    # IMAGES\n",
    "    # Shape: batch_size, seq_length, width, height\n",
    "    historical_sat_images: Array\n",
    "    target_sat_images: Array\n",
    "        \n",
    "    # METADATA\n",
    "    datetime_index: Array\n",
    "\n",
    "\n",
    "class BadData(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RandomSquareCrop():\n",
    "    size: int = 128  #: Size of the cropped image.\n",
    "\n",
    "    def __call__(self, sample: Sample) -> Sample:\n",
    "        crop_params = None\n",
    "        for attr_name in IMAGE_ATTR_NAMES:\n",
    "            image = sample[attr_name]\n",
    "            # TODO: Random crop!\n",
    "            cropped_image = image[..., :self.size, :self.size]\n",
    "            sample[attr_name] = cropped_image\n",
    "        return sample\n",
    "\n",
    "\n",
    "class CheckForBadData():\n",
    "    def __call__(self, sample: Sample) -> Sample:\n",
    "        for attr_name in IMAGE_ATTR_NAMES:\n",
    "            image = sample[attr_name]\n",
    "            if np.any(image < 0):\n",
    "                raise BadData(f'\\n{attr_name} has negative values at {image.time.values}')\n",
    "        return sample\n",
    "\n",
    "        \n",
    "class ToTensor():\n",
    "    def __call__(self, sample: Sample) -> Sample:\n",
    "        for key, value in sample.items():\n",
    "            if isinstance(value, xr.DataArray):\n",
    "                value = value.values\n",
    "            sample[key] = torch.from_numpy(value)\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "class Compose():\n",
    "    # Copied from https://pytorch.org/vision/stable/_modules/torchvision/transforms/transforms.html#Compose\n",
    "    # But not using torchvision, because it appears to create conda package conflicts\n",
    "    # with opencv?  But need to explore more!\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-mediterranean",
   "metadata": {},
   "source": [
    "## PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "knowing-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SatelliteDataset(torch.utils.data.IterableDataset):\n",
    "    zarr_chunk_sequences: Iterable[Segment]  #: Defines multiple Zarr chunks to be loaded from disk at once.\n",
    "    history_len: int = 1  #: The number of timesteps of 'history' to load.\n",
    "    forecast_len: int = 1  #: The number of timesteps of 'forecast' to load.\n",
    "    transform: Optional[Callable] = None\n",
    "    n_disk_loads_per_epoch: int = 10_000  #: Number of disk loads per worker process per epoch.\n",
    "    min_n_samples_per_disk_load: int = 1_000  #: Number of samples each worker will load for each disk load.\n",
    "    max_n_samples_per_disk_load: int = 2_000  #: Max number of disk loader.  Actual number is chosen randomly between min & max.\n",
    "    n_zarr_chunk_sequences_to_load_at_once: int = 8  #: Number of chunk seqs to load at once.  These are sampled at random.\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        #: Total sequence length of each sample.\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "    def per_worker_init(self, worker_id: int) -> None:\n",
    "        \"\"\"Called by worker_init_fn on each copy of SatelliteDataset after the worker process has been spawned.\"\"\"\n",
    "        print('Starting per_worker_init for worker', worker_id)\n",
    "        self.worker_id = worker_id\n",
    "        print('Starting to load sat data for worker', worker_id)\n",
    "        self.data_array = get_sat_data()\n",
    "        print('Finished loading sat data for worker', worker_id)\n",
    "        # Each worker must have a different seed for its random number generator.\n",
    "        # Otherwise all the workers will output exactly the same data!\n",
    "        seed = torch.initial_seed()\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "        print('Finished per_worker_init for worker', worker_id)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Asynchronously loads next data from disk while sampling from data_in_mem.\n",
    "        \"\"\"\n",
    "        with futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            future_data = executor.submit(self._load_data_from_disk)\n",
    "            for _ in range(self.n_disk_loads_per_epoch):\n",
    "                data_in_mem = future_data.result()\n",
    "                future_data = executor.submit(self._load_data_from_disk)\n",
    "                n_samples = self.rng.integers(self.min_n_samples_per_disk_load, self.max_n_samples_per_disk_load)\n",
    "                for _ in range(n_samples):\n",
    "                    sample = self._get_sample(data_in_mem)\n",
    "                    if self.transform:\n",
    "                        try:\n",
    "                            sample = self.transform(sample)\n",
    "                        except BadData as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "                    yield sample\n",
    "\n",
    "    def _load_data_from_disk(self) -> List[xr.DataArray]:\n",
    "        \"\"\"Loads data from contiguous Zarr chunks from disk into memory.\"\"\"\n",
    "        sat_images_list = []\n",
    "        for _ in range(self.n_zarr_chunk_sequences_to_load_at_once):\n",
    "            zarr_chunk_sequence = self.rng.choice(self.zarr_chunk_sequences)\n",
    "            sat_images = self.data_array.isel(time=slice(*zarr_chunk_sequence))\n",
    "\n",
    "            # Sanity checks\n",
    "            n_timesteps_available = len(sat_images)\n",
    "            if n_timesteps_available < self.total_seq_len:\n",
    "                raise RuntimeError(f'Not enough timesteps in loaded data!  Need at least {self.total_seq_len}.  Got {n_timesteps_available}!')\n",
    "\n",
    "            sat_images_list.append(sat_images.load())\n",
    "        return sat_images_list\n",
    "\n",
    "    def _get_sample(self, data_in_mem_list: List[xr.DataArray]) -> Sample:\n",
    "        i = self.rng.integers(0, len(data_in_mem_list))\n",
    "        data_in_mem = data_in_mem_list[i]\n",
    "        n_timesteps_available = len(data_in_mem)\n",
    "        max_start_idx = n_timesteps_available - self.total_seq_len\n",
    "        start_idx = self.rng.integers(low=0, high=max_start_idx, dtype=np.uint32)\n",
    "        end_idx = start_idx + self.total_seq_len\n",
    "        sat_images = data_in_mem.isel(time=slice(start_idx, end_idx))\n",
    "        return Sample(\n",
    "            historical_sat_images=sat_images[:self.history_len],\n",
    "            target_sat_images=sat_images[self.history_len:],\n",
    "            datetime_index=sat_images.time.values.astype('datetime64[s]').astype(int)\n",
    "        )\n",
    "\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"Configures each dataset worker process.\n",
    "    \n",
    "    Just has one job!  To call SatelliteDataset.per_worker_init().\n",
    "    \"\"\"\n",
    "    # get_worker_info() returns information specific to each worker process.\n",
    "    worker_info = torch.utils.data.get_worker_info()\n",
    "    if worker_info is None:\n",
    "        print('worker_info is None!')\n",
    "    else:\n",
    "        dataset_obj = worker_info.dataset  # The Dataset copy in this worker process.\n",
    "        dataset_obj.per_worker_init(worker_id=worker_info.id)\n",
    "        print('worker', worker_info.id, 'initialised!')\n",
    "\n",
    "\n",
    "torch.manual_seed(42)    \n",
    "\n",
    "dataset = SatelliteDataset(\n",
    "    zarr_chunk_sequences=zarr_chunk_sequences,\n",
    "    transform=Compose([\n",
    "        RandomSquareCrop(),\n",
    "        CheckForBadData(),\n",
    "        ToTensor(),\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "completed-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,  # Was 8!\n",
    "    num_workers=8,  # Was 8!  # timings:  4=13.8s; 8=11.6; 10=11.3s; 11=11.5s; 12=12.6s.  10=3it/s\n",
    "    worker_init_fn=worker_init_fn,\n",
    "    pin_memory=False,\n",
    "    #persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "expired-prime",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 67567) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/predict_pv_yield/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/predict_pv_yield/lib/python3.8/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/predict_pv_yield/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/predict_pv_yield/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/predict_pv_yield/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/predict_pv_yield/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/predict_pv_yield/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 67567) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/predict_pv_yield/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/predict_pv_yield/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/predict_pv_yield/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/predict_pv_yield/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 67567) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for batch in dataloader:\n",
    "    print(batch['historical_sat_images'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "uniform-madonna",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9aecc8d2a794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datetime64[s]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "pd.to_datetime(batch['datetime_index'].numpy().flatten(), unit='s').values.reshape(-1, 2).astype('datetime64[s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['historical_sat_images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['target_sat_images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['historical_sat_images'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch['historical_sat_images'][4, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-northwest",
   "metadata": {},
   "source": [
    "# Simple ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_images_in_model(images, device):    \n",
    "    SAT_IMAGE_MEAN = torch.tensor(93.23458, dtype=torch.float, device=device)\n",
    "    SAT_IMAGE_STD = torch.tensor(115.34247, dtype=torch.float, device=device)\n",
    "    \n",
    "    images = images.float()\n",
    "    images -= SAT_IMAGE_MEAN\n",
    "    images /= SAT_IMAGE_STD\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = 32\n",
    "KERNEL = 3\n",
    "\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_conv1 = nn.Conv2d(in_channels=1, out_channels=CHANNELS//2, kernel_size=KERNEL)\n",
    "        self.encoder_conv2 = nn.Conv2d(in_channels=CHANNELS//2, out_channels=CHANNELS, kernel_size=KERNEL)\n",
    "        self.encoder_conv3 = nn.Conv2d(in_channels=CHANNELS, out_channels=CHANNELS, kernel_size=KERNEL)\n",
    "        self.encoder_conv4 = nn.Conv2d(in_channels=CHANNELS, out_channels=CHANNELS, kernel_size=KERNEL)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=KERNEL)\n",
    "        \n",
    "        self.decoder_conv1 = nn.ConvTranspose2d(in_channels=CHANNELS, out_channels=CHANNELS, kernel_size=KERNEL)\n",
    "        self.decoder_conv2 = nn.ConvTranspose2d(in_channels=CHANNELS, out_channels=CHANNELS//2, kernel_size=KERNEL)\n",
    "        self.decoder_conv3 = nn.ConvTranspose2d(in_channels=CHANNELS//2, out_channels=CHANNELS//2, kernel_size=KERNEL)\n",
    "        self.decoder_conv4 = nn.ConvTranspose2d(in_channels=CHANNELS//2, out_channels=1, kernel_size=KERNEL)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        images = x['historical_sat_images']\n",
    "        images = normalise_images_in_model(images, self.device)\n",
    "        \n",
    "        # Pass data through the network :)\n",
    "        # ENCODER\n",
    "        out = F.relu(self.encoder_conv1(images))        \n",
    "        out = F.relu(self.encoder_conv2(out))       \n",
    "        out = F.relu(self.encoder_conv3(out))\n",
    "        out = F.relu(self.encoder_conv4(out))\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        # DECODER\n",
    "        out = F.relu(self.decoder_conv1(out))        \n",
    "        out = F.relu(self.decoder_conv2(out))        \n",
    "        out = F.relu(self.decoder_conv3(out))\n",
    "        out = self.decoder_conv4(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _training_or_validation_step(self, batch, is_train_step):\n",
    "        y_hat = self(batch)\n",
    "        y = batch['target_sat_images']\n",
    "        y = normalise_images_in_model(y, self.device)\n",
    "        y = y[..., 40:-40, 40:-40]  # Due to the CNN stride, the output image is 48 x 48\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        tag = \"Loss/Train\" if is_train_step else \"Loss/Validation\"\n",
    "        self.log_dict({tag: loss}, on_step=is_train_step, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._training_or_validation_step(batch, is_train_step=True)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._training_or_validation_step(batch, is_train_step=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitAutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=400, terminate_on_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.fit(model, train_dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-territory",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_pv_yield",
   "language": "python",
   "name": "predict_pv_yield"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
