{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "married-projection",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Our ultimate aim is to predict solar electricity power generation over the next few hours.\n",
    "\n",
    "## Loading terabytes of data efficiently from cloud storage\n",
    "\n",
    "We have several TB of satellite data. To keep the GPU fed with data during training, we need to read chunks of data quickly from the Zarr store; and we also want to load data asynchronously.  That is, while the GPU is training on the current batch, the data loader should simultaneously load the _next_ batch from disk.\n",
    "\n",
    "PyTorch makes this easy!  PyTorch's `DataLoader` spawns multiple worker processes when constructed with `num_workers` set to more than 1.  Each worker process receives a copy of the `SatelliteDataset` object.\n",
    "\n",
    "There is a small challenge: The code hangs when it gets to `enumerate(dataloader)` if we open the `xarray.DataArray` in the main process and copy that opened `DataArray` to the child processes.  Our solution is to delay the creation of the `DataArray` until _after_ the worker processes have been created.  PyTorch makes this easy by allowing us to pass a `worker_init_fn` to `DataLoader`.  `worker_init_fn` is called on each worker process.  Our `worker_init_fn` just has one job: to call `SatelliteDataset.per_worker_init()` which, in turn, opens the `DataArray`.\n",
    "\n",
    "This approach achieves read speeds of 600 MB/s from Google Cloud Storage to a single GCP VM with 12 vCPUs (as measured by `nethogs`).\n",
    "\n",
    "We use `IterableDataset` instead of `Dataset` so `SatelliteDataset` can pre-load the next example from disk and then block (on the `yield`) waiting for PyTorch to read that data.  This allows the worker processes to be processing the next data samples while the main process is training the current batch on the GPU.\n",
    "\n",
    "We can't pin the memory in each worker process because pinned memory can't be shared across processes.  Instead we ask `DataLoader` to pin the collated batch so that pytorch-lightning can asynchronously load the next batch from pinned CPU memory into GPU memory.\n",
    "\n",
    "The satellite data is stored on disk as `int16`.  To speed up the movement of the satellite data between processes and from the CPU memory to the GPU memory, we keep the data as `int16` until the data gets to the GPU, where it is converted to `float32` and normalised.\n",
    "\n",
    "### Loading data from disk into memory in chunks\n",
    "\n",
    "Cloud storage buckets can't seek into files like 'proper' POSIX filesystems can.  So, even if we just want 1 byte from a 1 GB file, we have to load the entire 1 GB file from the bucket.\n",
    "\n",
    "Zarr is designed with this challenge in mind.  Zarr gets round the inability of cloud storage buckets to seek by chunking the data into lots of small files.  But, still, we have to load entire Zarr chunks at once, even if we only want a part of a chunk.  And, even though we can pull 600 MB/s from a cloud storage bucket, the reads from the storage bucket are still the rate-limiting-step.  (GPUs are very fast and have a voracious appetite for data!)\n",
    "\n",
    "To get the most out of each disk read, our worker processes load several contiguous chunks of Zarr data from disk into memory at once.  We then randomly sample from the in-memory data multiple times, before loading another set of chunks from disk into memory.   This trick increases training speed by about 10x.\n",
    "\n",
    "Each Zarr chunk is 36 timesteps long and contains the entire geographical extent.  Each timestep is about 5 minutes apart, so each Zarr chunk spans 1.5 hours, assuming the timesteps are contiguous (more on contiguous chunks later).\n",
    "\n",
    "### Loading only daylight data\n",
    "\n",
    "We're interested in forecasting solar power generation, so we don't care about nighttime data :)\n",
    "\n",
    "In the UK in summer, the sun rises first in the north east, and sets last in the north west (see [video of June 2019](https://www.youtube.com/watch?v=IOp-tj-IJpk&t=0s)).  In summer, the north gets more hours of sunshine per day.\n",
    "\n",
    "In the UK in winter, the sun rises first in the south east, and sets last in the south west (see [video of Jan 2019](https://www.youtube.com/watch?v=CJ4prUVa2nQ)).  In winter, the south gets more hours of sunshine per day.\n",
    "\n",
    "|                        | Summer | Winter |\n",
    "|           ---:         |  :---: |  :---: |\n",
    "| Sun rises first in     | N.E.   | S.E.   |\n",
    "| Sun sets last in       | N.W.   | S.W.   |\n",
    "| Most hours of sunlight | North  | South  |\n",
    "\n",
    "We always load a pre-defined number of Zarr chunks from disk every disk load (defined by `n_chunks_per_disk_load`).\n",
    "\n",
    "Before training, we select timesteps which have at least some sunlight.  We do this by computing the clearsky global horizontal irradiance (GHI) for the four corners of the satellite imagery, and for all the timesteps in the dataset.  We only use timesteps where the maximum global horizontal irradiance across all four corners is above some threshold.\n",
    "\n",
    "(The 'clearsky [solar irradiance](https://en.wikipedia.org/wiki/Solar_irradiance)' is the amount of sunlight we'd expect on a clear day at a specific time and location. The SI unit of irradiance is watt per square meter.  The 'global horizontal irradiance' is the total sunlight that would hit a horizontal surface on the surface of the Earth.  The GHI is the sum of the direct irradiance (sunlight which takes a direct path from the Sun to the Earth's surface) and the diffuse horizontal irradiance (the sunlight scattered from the atmosphere)).\n",
    "\n",
    "### Finding contiguous sequences\n",
    "\n",
    "Once we have a list of 'lit' timesteps, we then find contiguous sequences (timeseries without any gaps).  And we then compute a list of contiguous Zarr chunks that we'll load at once during training.\n",
    "\n",
    "### Loading data during training\n",
    "\n",
    "During training, each worker process randomly picks multiple contiguous Zarr chunk sequences from the list of contiguous sequences pre-computed before training started.  The worker loads that data into memory and then randomly samples many samples from that in-memory data before loading more data from disk.\n",
    "\n",
    "#### Ensuring each batch contains a random sample of the dataset\n",
    "\n",
    "When PyTorch's `DataLoader` constructs a batch, it reads from just one worker process.  (This is not how I had _assumed_ it would work:  I assumed PyTorch would construct each batch by randomly sampling from all workers.)  This is an issue because, for stochastic gradient descent to work correctly, each batch must contain random samples of the dataset.  So it's not sufficient for each worker to load just one contiguous Zarr chunk (because then each batch would be made up entirely of samples from roughly the same time of day).  So, instead, each worker process loads multiple contiguous Zarr sequences into memory.  This also means that each worker must load quite a lot of data from disk.  To avoid training pausing while a worker process loads more data from disk, the data loading is done asynchronously using a separate thread within each worker process.\n",
    "\n",
    "## Timestep numbering:\n",
    "\n",
    "* t<sub>0</sub> is 'now': the most recent observation.\n",
    "* t<sub>1</sub> is the first timestep of the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adequate-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python core\n",
    "from typing import Optional, Callable, TypedDict, Union, Iterable, Tuple, NamedTuple, List\n",
    "from dataclasses import dataclass\n",
    "import datetime\n",
    "from itertools import product\n",
    "from concurrent import futures\n",
    "from pathlib import Path\n",
    "import numbers\n",
    "\n",
    "# Scientific python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numcodecs\n",
    "import matplotlib.pyplot as plt\n",
    "import dask\n",
    "\n",
    "# Cloud compute\n",
    "import gcsfs\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# PV & geospatial\n",
    "import pvlib\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf9f0a88-1adb-49ef-903e-fdb5b9e4793a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1905e1e5-c3ad-4ba5-a937-a03ecbb5103f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcsfs.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7896eb75-719b-4479-bd86-bc6b8e3e2352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbcfc691-e7d7-47b4-9def-a72c7e05b157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "605964ed-8f7c-4f35-a9b4-24f974dd24bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyproj.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-rainbow",
   "metadata": {},
   "source": [
    "## Consts & config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-humor",
   "metadata": {},
   "source": [
    "The [Zarr docs](https://zarr.readthedocs.io/en/stable/tutorial.html#configuring-blosc) say we should tell the Blosc compression library not to use threads because we're using multiple processes to read from our Zarr store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "complete-perry",
   "metadata": {},
   "outputs": [],
   "source": [
    "numcodecs.blosc.use_threads = False\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (25, 10)\n",
    "plt.rcParams['image.interpolation'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6ee0d3b-1a82-4f79-9831-4a7015ac3a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = Path('solar-pv-nowcasting-data')\n",
    "\n",
    "# Satellite data\n",
    "SAT_DATA_ZARR = BUCKET / 'satellite/EUMETSAT/SEVIRI_RSS/OSGB36/all_zarr_int16'\n",
    "\n",
    "# Solar PV data\n",
    "PV_PATH = BUCKET / 'PV/PVOutput.org'\n",
    "PV_DATA_FILENAME = PV_PATH / 'UK_PV_timeseries_batch.nc'\n",
    "PV_METADATA_FILENAME = PV_PATH / 'UK_PV_metadata.csv'\n",
    "\n",
    "# Numerical weather predictions\n",
    "NWP_ZARR = BUCKET / 'NWP/UK_Met_Office/UKV_zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e00fe030-9d63-4836-8fdb-21e64c7f50db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('solar-pv-nowcasting-data/PV/PVOutput.org/UK_PV_timeseries_batch.nc')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PV_DATA_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-quarterly",
   "metadata": {},
   "source": [
    "# Load satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fifteen-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sat_data(filename: Union[str, Path]=SAT_DATA_ZARR) -> xr.DataArray:\n",
    "    \"\"\"Lazily opens the Zarr store on Google Cloud Storage (GCS).\n",
    "    \n",
    "    Selects the High Resolution Visible (HRV) satellite channel.\n",
    "    \"\"\"\n",
    "    #gcs = gcsfs.GCSFileSystem(access='read_only')\n",
    "    #store = gcsfs.GCSMap(root=filename, gcs=gcs)\n",
    "    store = 'gs://' + str(filename)\n",
    "    dataset = xr.open_zarr(store, consolidated=True)\n",
    "    data_array = dataset['stacked_eumetsat_data'].sel(variable='HRV')\n",
    "    #gcs.clear_instance_cache()  # See https://github.com/dask/gcsfs/issues/379#issuecomment-826930203\n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fe0be8d-de3e-41a2-a2bd-2ec7cb07d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sat_data.close(); del sat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sat_data = get_sat_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-draft",
   "metadata": {},
   "source": [
    "Caution: Wierdly, plotting `sat_data` at this point causes the code to hang (with no errors messages) when it gets to `for batch in dataloader:`.  The code hangs even if we first do `sat_data.close(); del sat_data`.  See https://github.com/dask/gcsfs/issues/379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sat_data.variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-filing",
   "metadata": {},
   "source": [
    "Get the datetime boundaries of each Zarr chunk.  Later, we will use these boundaries to ensure we load complete chunks at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get the integer indicies of the chunk boundaries.\n",
    "zarr_chunk_boundaries = np.concatenate(([0], np.cumsum(sat_data.chunks[0])))\n",
    "\n",
    "# Then convert to datetimes.  Except the last one, because that's one-out-of-range.\n",
    "zarr_chunk_boundaries = pd.DatetimeIndex(\n",
    "    np.concatenate((sat_data['time'].values[zarr_chunk_boundaries[:-1]], sat_data['time'].values[-1:])), \n",
    "    tz='UTC')\n",
    "\n",
    "assert zarr_chunk_boundaries[-2] != zarr_chunk_boundaries[-1]\n",
    "\n",
    "zarr_chunk_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-continent",
   "metadata": {},
   "source": [
    "## Select daylight hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OSGB is also called \"OSGB 1936 / British National Grid -- United Kingdom Ordnance Survey\".\n",
    "# OSGB is used in many UK electricity system maps, and is used by the UK Met Office UKV model.\n",
    "# OSGB is a Transverse Mercator projection, using 'easting' and 'northing' coordinates\n",
    "# which are in meters.\n",
    "OSGB = 27700\n",
    "\n",
    "# WGS84 is short for \"World Geodetic System 1984\", used in GPS. Uses latitude and longitude.\n",
    "WGS84 = 4326\n",
    "\n",
    "# osgb_to_wgs84.transform(x, y) returns latitude (north-south), longitude (east-west)\n",
    "osgb_to_wgs84 = pyproj.Transformer.from_crs(crs_from=OSGB, crs_to=WGS84)\n",
    "\n",
    "# wgs84_to_osgb.transform(lat, lon) returns x, y\n",
    "wgs84_to_osgb = pyproj.Transformer.from_crs(crs_from=WGS84, crs_to=OSGB)\n",
    "\n",
    "def select_daylight_timestamps(\n",
    "    dt_index: pd.DatetimeIndex, \n",
    "    locations: Iterable[Tuple[float, float]],\n",
    "    ghi_threshold: float = 10\n",
    "    ) -> pd.DatetimeIndex:\n",
    "    \"\"\"Returns datetimes for which the global horizontal irradiance\n",
    "    (GHI) is above ghi_threshold across all locations.\n",
    "\n",
    "    Args:\n",
    "      dt_index: DatetimeIndex to filter.  Must be UTC.\n",
    "      locations: List of Tuples of x, y coordinates in OSGB projection.\n",
    "      ghi_threshold: Global horizontal irradiance threshold.  (Watts per square meter?)\n",
    "    \"\"\"\n",
    "    assert dt_index.tz.zone == 'UTC'\n",
    "    ghi_for_all_locations = []\n",
    "    for x, y in locations:\n",
    "        lat, lon = osgb_to_wgs84.transform(x, y)\n",
    "        location = pvlib.location.Location(latitude=lat, longitude=lon)\n",
    "        clearsky = location.get_clearsky(dt_index)\n",
    "        ghi = clearsky['ghi']\n",
    "        ghi_for_all_locations.append(ghi)\n",
    "        \n",
    "    ghi_for_all_locations = pd.concat(ghi_for_all_locations, axis='columns')\n",
    "    max_ghi = ghi_for_all_locations.max(axis='columns')\n",
    "    mask = max_ghi > ghi_threshold\n",
    "    return dt_index[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77876830-066c-4b0c-95a5-82c48ca36a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, lon = osgb_to_wgs84.transform(100_000, 50_000)\n",
    "lat, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c7490-6133-412a-b194-02c40aedac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = wgs84_to_osgb.transform(lat, lon)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 'corner' coordinates for a rectange GEO_BORDER within the actual boundary of the satellite imagery.\n",
    "GEO_BORDER: int = 64  #: In same geo projection and units as sat_data.\n",
    "corners = [\n",
    "    (sat_data.x.values[x], sat_data.y.values[y]) \n",
    "    for x, y in product(\n",
    "        [GEO_BORDER, -GEO_BORDER], \n",
    "        [GEO_BORDER, -GEO_BORDER])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "datetimes = select_daylight_timestamps(\n",
    "    dt_index=pd.DatetimeIndex(sat_data.time.values, tz='UTC'), \n",
    "    locations=corners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e32bbdc-49ea-45a4-8272-84f84f1e406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datetimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7871d1-327f-4b5a-aa8e-d53a1ace48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sat_data.close()\n",
    "#del sat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f53e154-eb7a-4405-868b-42bd3931f63e",
   "metadata": {},
   "source": [
    "# Load PV power timeseries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6609f6d-e8de-4c30-a80f-27205c850ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pv_metadata = pd.read_csv(f'gs://{PV_METADATA_FILENAME}', index_col='system_id')\n",
    "pv_metadata.dropna(subset=['longitude', 'latitude'], how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ce35a-79cf-439c-ace4-30668facfe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132fa8ba-67f5-490d-b3eb-7b26b30eb682",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_metadata['location_x'], pv_metadata['location_y'] = wgs84_to_osgb.transform(pv_metadata['latitude'], pv_metadata['longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbcb42e-98f2-48fa-a6f4-40013ff09741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove PV systems outside of the geospatial extent of the satellite data\n",
    "GEOSPATIAL_BOUNDARY = {\n",
    "    'WEST': sat_data.x.values[0],\n",
    "    'EAST': sat_data.x.values[-1],\n",
    "    'NORTH': sat_data.y.values[0],\n",
    "    'SOUTH': sat_data.y.values[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064a4e0-c8d6-4dcd-b5d5-5dd6e59b6d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOSPATIAL_BOUNDARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1aed1f-ad97-44e8-befd-65dc0f4c03c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_metadata = pv_metadata[\n",
    "    (pv_metadata.location_x >= GEOSPATIAL_BOUNDARY['WEST']) &\n",
    "    (pv_metadata.location_x <= GEOSPATIAL_BOUNDARY['EAST']) &\n",
    "    (pv_metadata.location_y <= GEOSPATIAL_BOUNDARY['NORTH']) &\n",
    "    (pv_metadata.location_y >= GEOSPATIAL_BOUNDARY['SOUTH'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc0e55-dc7c-457f-85c1-99969a792b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f38d3-f259-4db5-8b51-b03cf08f9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_solar_pv_data(start_datetime, end_datetime) -> pd.DataFrame:\n",
    "    gcs = gcsfs.GCSFileSystem(access='read_only')\n",
    "\n",
    "    # It is possible to simplify the code below and do the xr.open_dataset(file) in\n",
    "    # the first 'with' block, and delete the second 'with' block.  But that takes 6 minutes to load the data, \n",
    "    # where as loading into memory first and then loading from there takes 23 seconds!\n",
    "    with gcs.open(PV_DATA_FILENAME, mode='rb') as file:\n",
    "        file_bytes = file.read()\n",
    "\n",
    "    with io.BytesIO(file_bytes) as file:\n",
    "        pv_power = xr.open_dataset(file)\n",
    "        pv_power_df = pv_power.sel(datetime=slice(start_datetime, end_datetime)).to_dataframe()\n",
    "\n",
    "    # Save memory\n",
    "    del file_bytes    \n",
    "    del pv_power\n",
    "    \n",
    "    # Tidy up\n",
    "    gcs.clear_instance_cache()  # See https://github.com/dask/gcsfs/issues/379#issuecomment-826930203\n",
    "    \n",
    "    # Process the data a little\n",
    "    pv_power_df = pv_power_df.dropna(axis='columns', how='all')\n",
    "    pv_power_df = pv_power_df.clip(lower=0, upper=5E7)\n",
    "    pv_power_df.columns = [np.int32(col) for col in pv_power_df.columns]    \n",
    "    return pv_power_df.tz_localize('Europe/London').tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16fbc7a-ce47-419a-9b61-3255873c2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Clip to start and end date of satellite data.\n",
    "# Note that the data in the net CDF file is in the Europe/London timezone,\n",
    "# but xarray can only handle timezone-naive timestamps.\n",
    "datetimes_naive = datetimes.tz_convert('Europe/London').tz_convert(None)\n",
    "pv_power_df = load_solar_pv_data(\n",
    "    start_datetime=datetimes_naive[0],\n",
    "    end_datetime=datetimes_naive[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3efeb-4438-4fe9-9e19-74696f35621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4afc98d-e966-4ea8-a782-9e0677cfb7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_df[10003].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6516b87-f7c8-4b27-b268-e02f4386a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_df.values.nbytes / 1E9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fbc87e-626b-4bbc-affc-32af5046f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of hand-crafted cleaning\n",
    "pv_power_df[30248]['2018-10-29':'2019-01-03'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191d7503-dda4-4c63-8832-61210d2de4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only pick PV systems for which we have metadata.\n",
    "def align_pv_system_ids(pv_metadata, pv_power_df):\n",
    "    pv_system_ids = pv_metadata.index.intersection(pv_power_df.columns)\n",
    "    pv_system_ids = np.sort(pv_system_ids)\n",
    "\n",
    "    pv_power_df = pv_power_df[pv_system_ids]\n",
    "    pv_metadata = pv_metadata.loc[pv_system_ids]\n",
    "    return pv_metadata, pv_power_df\n",
    "    \n",
    "pv_metadata, pv_power_df = align_pv_system_ids(pv_metadata, pv_power_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b351b6c-9189-4249-b2eb-7c348953d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pv_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeeadd5-8985-4fb4-9cb0-27e41387f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale to the range [0, 1]\n",
    "pv_power_min = pv_power_df.min()\n",
    "pv_power_df -= pv_power_min\n",
    "pv_power_max = pv_power_df.max()\n",
    "pv_power_df /= pv_power_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8b344-361e-445c-a9b3-3b564d75240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_df.min().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee3ba58-ed6c-4575-ac90-eaad35991379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop systems which produce power over night\n",
    "NIGHT_YIELD_THRESHOLD = 0.4\n",
    "night_hours = [22, 23, 0, 1, 2]\n",
    "pv_data_at_night = pv_power_df.loc[pv_power_df.index.hour.isin(night_hours)]\n",
    "pv_above_threshold_at_night = (pv_data_at_night > NIGHT_YIELD_THRESHOLD).any()\n",
    "bad_systems = pv_power_df.columns[pv_above_threshold_at_night]\n",
    "print(len(bad_systems), 'bad systems found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f18407-4913-4110-bee7-76eeeaea0dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Of these bad systems, 24647, 42656, 42807, 43081, 51247, 59919 might have some salvagable data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404ebd9-778d-4d80-8c78-c42bc14db93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_df = pv_power_df.drop(columns=bad_systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827c0084-8c05-4efe-ba60-65e08bf1c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Resample to 5-minutely and interpolate up to 15 minutes ahead.\n",
    "pv_power_df = pv_power_df.resample('5T').interpolate(method='time', limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8c635-a8a8-4083-87b5-e596948c6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_df[pv_power_df.columns[5]]['2019-01'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba8f7b-ad78-4605-802a-a689b891fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align again, after removing dud PV systems\n",
    "pv_metadata, pv_power_df = align_pv_system_ids(pv_metadata, pv_power_df)\n",
    "len(pv_power_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a176dca-214b-4f54-8514-90b7855e643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pv_power_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68915282-515c-405f-97e9-759f05d6ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select datetimes where there is data for at least 1 PV system\n",
    "pv_power_df = pv_power_df.reindex(datetimes + pd.Timedelta('1 minute')).dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade847d-7fdb-4caf-baec-573d2d4a5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pv_power_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40e149-51e8-481c-9607-bcf001b3ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datetimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34038a9e-fc3d-4f2a-bb30-9bf48cb3d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = pv_power_df.index - pd.Timedelta('1 minute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3b4db6-0df7-4116-82ab-aad868d7f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datetimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a5bfe-ee51-4561-b333-25f585353195",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_data.sel(time=datetimes[20]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05dc341-4347-43a5-8bf8-1b0ee0b79950",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_df.values.nbytes / 1E6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335878d8-c8fc-48a7-a127-92e3e4744c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum number of PV systems available for one timestep:\n",
    "(~pv_power_df.isna()).sum(axis='columns').min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0867ab-2422-4a79-a516-becd76006f3a",
   "metadata": {},
   "source": [
    "# Load numerical weather predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4373ee-0aaf-4229-8447-95c4f023405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nwp(base_path: Path = NWP_ZARR):\n",
    "    nwp_datasets = []\n",
    "    for zarr_store in ['2018_1-6', '2018_7-12', '2019_1-6', '2019_7-12']:\n",
    "        full_dir = base_path / zarr_store\n",
    "        full_dir = 'gs://' + str(full_dir)\n",
    "        print(full_dir)\n",
    "        dataset = xr.open_zarr(full_dir, consolidated=True)\n",
    "        dataset = dataset.rename({'time': 'init_time'})\n",
    "        nwp_datasets.append(dataset)\n",
    "\n",
    "    # The isobaricInhPa coordinates look messed up, especially in the 2018_7-12 and 2019_7-12 Zarr stores.\n",
    "    # So let's drop all the variables with multiple vertical levels for now:\n",
    "    for ds in nwp_datasets:\n",
    "        del ds['isobaricInhPa'], ds['gh_p'], ds['r_p'], ds['t_p'], ds['wdir_p'], ds['ws_p']\n",
    "\n",
    "    # Concat.\n",
    "    dask.config.set({\"array.slicing.split_large_chunks\": False})  # Silence warning about large chunks\n",
    "    nwp = xr.concat(nwp_datasets, dim='init_time')\n",
    "    \n",
    "    # There are a lot of doubled-up indicies from 2018-07-18 00:00 to 2018-08-27 09:00.\n",
    "    # De-duplicate the index. Code adapted from https://stackoverflow.com/a/51077784/732596\n",
    "    _, unique_index = np.unique(nwp.init_time, return_index=True)\n",
    "    return nwp.isel(init_time=unique_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1df6e-20dd-43c9-bc4c-7e37b1090c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nwp = load_nwp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f0e5a-3bf3-41d2-81a8-b8fbee537fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of the NWP variables\n",
    "\n",
    "nwp_var_names = [var_name for var_name, _ in nwp.variables.items()]\n",
    "\n",
    "nwp_var_details = pd.DataFrame(columns=['name', 'units'], index=nwp_var_names)\n",
    "for var_name, var in nwp.variables.items():\n",
    "    attrs = var.attrs\n",
    "    if 'stanard_name' in attrs:\n",
    "        name = attrs['standard_name']\n",
    "    elif 'long_name' in attrs:\n",
    "        name = attrs['long_name']\n",
    "    else:\n",
    "        name = ''\n",
    "        \n",
    "    try:\n",
    "        units = var.attrs['units']\n",
    "    except:\n",
    "        units = ''\n",
    "    nwp_var_details.loc[var_name] = {'name': name, 'units': units}\n",
    "    \n",
    "nwp_var_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66432c-3ab8-4264-aee7-2e8e9f6547c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 20))\n",
    "\n",
    "ax = axes[0][0]\n",
    "nwp['lcc'].sel(init_time='2018-06-01T12:00').isel(step=0).clip(0).plot(ax=ax)\n",
    "\n",
    "ax = axes[0][1]\n",
    "nwp['mcc'].sel(init_time='2018-06-01T12:00').isel(step=0).clip(0).plot(ax=ax)\n",
    "\n",
    "ax = axes[1][0]\n",
    "nwp['hcc'].sel(init_time='2018-06-01T12:00').isel(step=0).clip(0).plot(ax=ax)\n",
    "\n",
    "ax = axes[1][1]\n",
    "sat_data.sel(time='2018-06-01T12:04').plot(ax=ax)\n",
    "\n",
    "ax = axes[2][0]\n",
    "nwp['dswrf'].sel(init_time='2018-06-01T12:00').isel(step=0).plot(ax=ax)\n",
    "\n",
    "ax = axes[2][1]\n",
    "nwp['dlwrf'].sel(init_time='2018-06-01T12:00').isel(step=0).plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-effort",
   "metadata": {},
   "source": [
    "## Get contiguous segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segment(NamedTuple):\n",
    "    \"\"\"Represents the start and end datetimes of a segment of contiguous samples.\n",
    "    \n",
    "    The Segment covers the range [start, end].\n",
    "    \"\"\"\n",
    "    start: pd.Timestamp\n",
    "    end: pd.Timestamp\n",
    "        \n",
    "    def to_naive(self):\n",
    "        start = self.start.tz_convert('UTC').tz_convert(None)\n",
    "        end = self.end.tz_convert('UTC').tz_convert(None)\n",
    "        return Segment(start, end)\n",
    "\n",
    "\n",
    "def get_contiguous_segments(dt_index: pd.DatetimeIndex, min_timesteps: int, max_gap: pd.Timedelta) -> List[Segment]:\n",
    "    \"\"\"Chunk datetime index into contiguous segments, each at least min_timesteps long.\n",
    "    \n",
    "    max_gap defines the threshold for what constitutes a 'gap' between contiguous segments.\n",
    "    \n",
    "    Throw away any timesteps in a sequence shorter than min_timesteps long.\n",
    "    \"\"\"\n",
    "    gap_mask = np.diff(dt_index) > max_gap\n",
    "    gap_indices = np.argwhere(gap_mask)[:, 0]\n",
    "\n",
    "    # gap_indicies are the indices into dt_index for the timestep immediately before the gap.\n",
    "    # e.g. if the datetimes at 12:00, 12:05, 18:00, 18:05 then gap_indicies will be [1].\n",
    "    segment_boundaries = gap_indices + 1\n",
    "\n",
    "    # Capture the last segment of dt_index.\n",
    "    segment_boundaries = np.concatenate((segment_boundaries, [len(dt_index)]))\n",
    "\n",
    "    assert segment_boundaries[-2] != segment_boundaries[-1]\n",
    "    \n",
    "    segments = []\n",
    "    start_i = 0\n",
    "    for end_i in segment_boundaries:\n",
    "        n_timesteps = end_i - start_i\n",
    "        if n_timesteps >= min_timesteps:\n",
    "            segment = Segment(start=dt_index[start_i], end=dt_index[end_i-1])\n",
    "            segments.append(segment)\n",
    "        start_i = end_i\n",
    "        \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "contiguous_segments = get_contiguous_segments(\n",
    "    dt_index = datetimes,\n",
    "    min_timesteps = 36 * 1.5,\n",
    "    max_gap = pd.Timedelta('5 minutes'))\n",
    "\n",
    "contiguous_segments[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contiguous_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-diagram",
   "metadata": {},
   "source": [
    "## Turn the contiguous segments into sequences of Zarr chunks, which will be loaded together during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zarr_chunk_sequences(\n",
    "    n_chunks_per_disk_load: int, \n",
    "    zarr_chunk_boundaries: List[datetime.datetime], \n",
    "    contiguous_segments: List[Segment]) -> List[Segment]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "      n_chunks_per_disk_load: Maximum number of Zarr chunks to load from disk in one go.\n",
    "      zarr_chunk_boundaries: The datetime indicies into the Zarr store's time dimension which define the Zarr chunk boundaries.\n",
    "        Must be sorted.\n",
    "      contiguous_segments: Datetime indicies into the Zarr store's time dimension that define contiguous timeseries.\n",
    "        That is, timeseries with no gaps.\n",
    "    \n",
    "    Returns zarr_chunk_sequences: a list of Segments representing the start and end datetimes of contiguous sequences of multiple Zarr chunks,\n",
    "    all exactly n_chunks_per_disk_load long (for contiguous segments at least as long as n_chunks_per_disk_load zarr chunks),\n",
    "    and at least one side of the boundary will lie on a 'natural' Zarr chunk boundary.\n",
    "    \n",
    "    For example, say that n_chunks_per_disk_load = 3, and the Zarr chunks sizes are all 5:\n",
    "    \n",
    "    \n",
    "                  0    5   10   15   20   25   30   35 \n",
    "                  |....|....|....|....|....|....|....|\n",
    "\n",
    "    INPUTS:\n",
    "                     |------CONTIGUOUS SEGMENT----|\n",
    "                     \n",
    "    zarr_chunk_boundaries:\n",
    "                  |----|----|----|----|----|----|----|\n",
    "    \n",
    "    OUTPUT:\n",
    "    zarr_chunk_sequences:\n",
    "           3 to 15:  |-|----|----|\n",
    "           5 to 20:    |----|----|----|\n",
    "          10 to 25:         |----|----|----|\n",
    "          15 to 30:              |----|----|----|\n",
    "          20 to 32:                   |----|----|-|\n",
    "    \n",
    "    \"\"\"\n",
    "    assert n_chunks_per_disk_load > 0\n",
    "    \n",
    "    zarr_chunk_sequences = []\n",
    "\n",
    "    for contig_segment in contiguous_segments:\n",
    "        # searchsorted() returns the index into zarr_chunk_boundaries at which contig_segment.start\n",
    "        # should be inserted into zarr_chunk_boundaries to maintain a sorted list.\n",
    "        # i_of_first_zarr_chunk is the index to the element in zarr_chunk_boundaries which defines\n",
    "        # the start of the current contig chunk.\n",
    "        i_of_first_zarr_chunk = np.searchsorted(zarr_chunk_boundaries, contig_segment.start)\n",
    "        \n",
    "        # i_of_first_zarr_chunk will be too large by 1 unless contig_segment.start lies\n",
    "        # exactly on a Zarr chunk boundary.  Hence we must subtract 1, or else we'll\n",
    "        # end up with the first contig_chunk being 1 + n_chunks_per_disk_load chunks long.\n",
    "        if zarr_chunk_boundaries[i_of_first_zarr_chunk] > contig_segment.start:\n",
    "            i_of_first_zarr_chunk -= 1\n",
    "            \n",
    "        # Prepare for looping to create multiple Zarr chunk sequences for the current contig_segment.\n",
    "        zarr_chunk_seq_start = contig_segment.start\n",
    "        zarr_chunk_seq_end = None  # Just a convenience to allow us to break the while loop by checking if zarr_chunk_seq_end != contig_segment.end.\n",
    "        while zarr_chunk_seq_end != contig_segment.end:\n",
    "            zarr_chunk_seq_end = zarr_chunk_boundaries[i_of_first_zarr_chunk + n_chunks_per_disk_load]\n",
    "            zarr_chunk_seq_end = min(zarr_chunk_seq_end, contig_segment.end)\n",
    "            zarr_chunk_sequences.append(Segment(start=zarr_chunk_seq_start, end=zarr_chunk_seq_end))\n",
    "            i_of_first_zarr_chunk += 1\n",
    "            zarr_chunk_seq_start = zarr_chunk_boundaries[i_of_first_zarr_chunk]\n",
    "            \n",
    "    return zarr_chunk_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_chunk_sequences = get_zarr_chunk_sequences(\n",
    "    n_chunks_per_disk_load=3,\n",
    "    zarr_chunk_boundaries=zarr_chunk_boundaries,\n",
    "    contiguous_segments=contiguous_segments)\n",
    "\n",
    "zarr_chunk_sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbeb8a1-d065-4b36-a3f8-5e105feece43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durations of zarr chunk sequences\n",
    "[segment.end - segment.start for segment in zarr_chunk_sequences[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-sponsorship",
   "metadata": {},
   "source": [
    "## PyTorch data storage & processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "Array = Union[np.ndarray, xr.DataArray]\n",
    "\n",
    "IMAGE_ATTR_NAMES = ('historical_sat_images', 'target_sat_images')\n",
    "\n",
    "class Sample(TypedDict):\n",
    "    \"\"\"Simple class for structuring data for the ML model.\n",
    "    \n",
    "    Using typing.TypedDict gives us several advantages:\n",
    "      1. Single 'source of truth' for the type and documentation of each example.\n",
    "      2. A static type checker can check the types are correct.\n",
    "\n",
    "    Instead of TypedDict, we could use typing.NamedTuple,\n",
    "    which would provide runtime checks, but the deal-breaker with Tuples is that they're immutable\n",
    "    so we cannot change the values in the transforms.\n",
    "    \"\"\"\n",
    "    # IMAGES\n",
    "    # Shape: batch_size, seq_length, width, height\n",
    "    historical_sat_images: Array\n",
    "    target_sat_images: Array\n",
    "        \n",
    "    # PV yield time series\n",
    "    historical_pv_yield: pd.Series\n",
    "    target_pv_yield: pd.Series\n",
    "        \n",
    "    # Numerical weather predictions (NWPs)\n",
    "    nwp_above_pv: Array  #: The NWP at a single point nearest to the PV system.\n",
    "    \n",
    "    # METADATA\n",
    "    forecast_len: int\n",
    "    history_len: int\n",
    "    pv_system_id: int\n",
    "    pv_system_row_number: int  #: Guaranteed to be in the range [0, len(pv_metadata)]\n",
    "    pv_location_x: float\n",
    "    pv_location_y: float\n",
    "    datetime_index: Array  #: At 5-minute timings like 00, 05, 10, ...; *not* the 04, 09, ... sequence of the satellite imagery.\n",
    "\n",
    "\n",
    "class BadData(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RandomSquareCrop():\n",
    "    size: int = 128  #: Size (in pixels) of the cropped image.\n",
    "\n",
    "    def __call__(self, sample: Sample) -> Sample:\n",
    "        for attr_name in IMAGE_ATTR_NAMES:\n",
    "            image = sample[attr_name]\n",
    "            # TODO: Random crop!\n",
    "            cropped_image = image[..., :self.size, :self.size]\n",
    "            sample[attr_name] = cropped_image\n",
    "        return sample\n",
    "\n",
    "\n",
    "def crop_square(data_array: xr.DataArray, centre_x_osgb: float, centre_y_osgb: float, size_pixels: int):\n",
    "    half_size_pixels = size_pixels // 2\n",
    "\n",
    "    # centre_y_osgb and centre_x_osgb are in OSGB-space; but size_pixels is number of pixels!\n",
    "    # Need to convert to integer index into image pixels.\n",
    "    # The y array is in _descending_ order.\n",
    "    centre_x_index = np.searchsorted(data_array.x, centre_x_osgb)\n",
    "    centre_y_index = np.searchsorted(data_array.y[::-1], centre_y_osgb)\n",
    "    centre_y_index = len(data_array.y) - centre_y_index\n",
    "    \n",
    "    # Get coordinates for boundaries of the cropped image.\n",
    "    north = centre_y_index - half_size_pixels\n",
    "    south = centre_y_index + half_size_pixels\n",
    "    east = centre_x_index + half_size_pixels\n",
    "    west = centre_x_index - half_size_pixels\n",
    "\n",
    "    cropped = data_array.isel(\n",
    "        x=slice(west, east), \n",
    "        y=slice(north, south))\n",
    "    \n",
    "    assert len(cropped.x) == size_pixels, len(cropped.x)\n",
    "    assert len(cropped.y) == size_pixels, len(cropped.y)\n",
    "    \n",
    "    return cropped\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class CropCentredOnPv():\n",
    "    size_pixels: int = 128  #: Size (in pixels) of the cropped squaure image.  Must be an even number.\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        assert self.size_pixels % 2 == 0\n",
    "    \n",
    "    def __call__(self, sample: Sample) -> Sample:\n",
    "        x = sample['pv_location_x']\n",
    "        y = sample['pv_location_y']\n",
    "        \n",
    "        for attr_name in IMAGE_ATTR_NAMES:\n",
    "            image = sample[attr_name]\n",
    "            cropped_image = crop_square(image, x, y, self.size_pixels)\n",
    "            sample[attr_name] = cropped_image\n",
    "        return sample\n",
    "\n",
    "\n",
    "class CheckForBadData():\n",
    "    def __call__(self, sample: Sample) -> Sample:\n",
    "        for attr_name in IMAGE_ATTR_NAMES:\n",
    "            image = sample[attr_name]\n",
    "            if np.any(image < 0):\n",
    "                raise BadData(f'\\n{attr_name} has negative values at {image.time.values}')\n",
    "        return sample\n",
    "\n",
    "        \n",
    "class ToTensor():\n",
    "    def __call__(self, sample: Sample) -> Sample:\n",
    "        for key, value in sample.items():\n",
    "            original_type = type(value)  # Helpful for debugging.\n",
    "            if isinstance(value, (xr.DataArray, pd.Series, pd.DataFrame)):\n",
    "                value = value.values\n",
    "            elif isinstance(value, pd.DatetimeIndex):\n",
    "                value = value.values.astype('datetime64[s]').astype(np.int64)\n",
    "            elif isinstance(value, numbers.Number):\n",
    "                value = np.asanyarray(value)\n",
    "            \n",
    "            try:\n",
    "                sample[key] = torch.from_numpy(value)\n",
    "            except:\n",
    "                print(f'Failed to convert {key}, with value of type {original_type} = {value}')\n",
    "                raise\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "class Compose():\n",
    "    # Copied from https://pytorch.org/vision/stable/_modules/torchvision/transforms/transforms.html#Compose\n",
    "    # But not using torchvision, because it appears to create conda package conflicts\n",
    "    # with opencv?  But need to explore more!\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-mediterranean",
   "metadata": {},
   "source": [
    "## PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d38ac9-8ba5-41c5-8bca-bf08f61182d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nwp_example(\n",
    "    nwp: xr.Dataset, \n",
    "    start: pd.Timestamp, \n",
    "    end: pd.Timestamp,\n",
    "    t0: pd.Timestamp,\n",
    "    freq: str='5T',\n",
    "    interpolation_method: str='cubic'\n",
    "    ) -> xr.Dataset:\n",
    "    \"\"\"Select the numerical weather predictions (NWP) for a single ML example.\n",
    "    \n",
    "    The NWP for each example covers a contiguous timespan running from `start` to `end`.\n",
    "    The first part of the timeseries [`start`, `t0`] is the 'recent history'.\n",
    "    The second part of the timeseries (`t0`, `end`] is the 'future'.\n",
    "    \n",
    "    For each timestep in the recent history [`start`, `t0`], get predictions \n",
    "    produced by the freshest NWP run to each timestep.\n",
    "    \n",
    "    For the future (`t0`, `end`], use the NWP initialised most recently to t0.\n",
    "    \n",
    "    This function will also resample the NWP using `freq` and `interpolation_method`.\n",
    "    \n",
    "    Args:\n",
    "      nwp: Numerical weather prediction Dataset.  Assumed to be hourly.\n",
    "      start, end: The start and end datetimes of the entire example.\n",
    "      t0_datetime: The datetime that represents 'now'.  That is, any datetimes <= 'now'\n",
    "        are considered recent history; and datetimes > 'now' are considered forecast.\n",
    "      freq: The frequency to resample to (e.g. '5T' for 5-minutely).\n",
    "      interpolation_method: The interpolation method to use when resampling.\n",
    "    \"\"\"\n",
    "    # First we get the hourly NWPs; then we resample to `freq` at the end of the function.\n",
    "    \n",
    "    # Extend the start and end of the timespan by 1 hour, so that the\n",
    "    # cubic interpolation has more 'context' to work with. \n",
    "    BUFFER = pd.Timedelta('1H')\n",
    "    start_hourly = start.floor('H') - BUFFER\n",
    "    t0_hourly = t0.ceil('H')\n",
    "    end_hourly = end.ceil('H') + BUFFER\n",
    "    \n",
    "    target_times_hourly = pd.date_range(start_hourly, end_hourly, freq='H')\n",
    "    \n",
    "    # Get the most recent NWP initialisation time for each target_time_hourly.\n",
    "    init_times = nwp.sel(init_time=target_times_hourly, method='ffill').init_time.values\n",
    "    \n",
    "    # Find the NWP init time for just the 'future' portion of the example.\n",
    "    init_time_future = init_times[target_times_hourly == t0_hourly]\n",
    "    \n",
    "    # For the 'future' portion of the example, replace all the NWP init times with the\n",
    "    # NWP init time most recent to t0.\n",
    "    init_times[target_times_hourly > t0_hourly] = init_time_future\n",
    "    \n",
    "    # steps is the number of hourly timesteps beyond the NWP initialisation time.\n",
    "    steps = target_times_hourly - init_times\n",
    "\n",
    "    def _get_data_array_indexer(index):\n",
    "        # We want one timestep for each target_time_hourly (obviously!)\n",
    "        # If we simply do nwp.sel(init_time=init_times, step=steps)\n",
    "        # then we'll get the *product* of init_times and steps,\n",
    "        # which is not what we want!  Instead, we use xarray's vectorized-indexing mode\n",
    "        # by using a DataArray indexer.  See the last example here:\n",
    "        # http://xarray.pydata.org/en/stable/user-guide/indexing.html#vectorized-indexing\n",
    "        return xr.DataArray(index, dims='target_time', coords={'target_time': target_times_hourly})\n",
    "    \n",
    "    init_time_indexer = _get_data_array_indexer(init_times)\n",
    "    step_indexer = _get_data_array_indexer(steps)\n",
    "    nwp_selected = nwp.sel(init_time=init_time_indexer, step=step_indexer)\n",
    "    nwp_selected = nwp_selected.resample({'target_time': freq}).interpolate(interpolation_method)\n",
    "    return nwp_selected.sel(target_time=slice(start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb188427-a3df-41fb-8c72-615b017c50df",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_nwp = get_nwp_example(\n",
    "    nwp,\n",
    "    start=pd.Timestamp('2018-04-01T05:00'),\n",
    "    end=pd.Timestamp('2018-04-01T07:00'),\n",
    "    t0=pd.Timestamp('2018-04-01T06:00')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515b879-fa0b-4349-92de-343bcf1928fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_nwp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406bd206-3959-4aca-9623-94eeffa17d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = selected_nwp['t'].sel(x=100, y=100, method='nearest').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource():\n",
    "    \"\"\"Base class for additional data sources to be added into the Satellite data.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.rng = np.random.default_rng()  # Will be replaced if per_worker_init is called.\n",
    "        \n",
    "    def per_worker_init(self, worker_id: int=0) -> None:\n",
    "        # Each worker must have a different seed for its random number generator.\n",
    "        # Otherwise all the workers will output exactly the same data!\n",
    "        seed = torch.initial_seed()\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    def __getitem__(self, sample: Sample) -> Sample:\n",
    "        \"\"\"The method that sub-classes should override.\n",
    "        \n",
    "        Typically, this method should set one or more attributes in the `sample` dictionary.\n",
    "        \"\"\"\n",
    "        raise NotImplemented()\n",
    "\n",
    "    \n",
    "@dataclass\n",
    "class PVDataSource(DataSource):\n",
    "    pv_power_df: pd.DataFrame = pv_power_df  #: PV yield, resampled to 5-minutely\n",
    "    pv_metadata: pd.DataFrame = pv_metadata\n",
    "        \n",
    "        \n",
    "    def __post_init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "    def __getitem__(self, sample: Sample) -> Sample:\n",
    "        datetime_index = sample['datetime_index']\n",
    "        history_len = sample['history_len']\n",
    "        \n",
    "        selected_pv_power_df = self.pv_power_df.loc[datetime_index]\n",
    "        \n",
    "        # Select just one PV system\n",
    "        selected_pv_power_df = selected_pv_power_df.dropna(axis='columns', how='any')\n",
    "        pv_system_ids = selected_pv_power_df.columns\n",
    "        pv_system_id = self.rng.choice(pv_system_ids)\n",
    "        selected_pv_power = selected_pv_power_df[pv_system_id]\n",
    "        \n",
    "        # Get metadata for PV system\n",
    "        metadata_for_pv_system = self.pv_metadata.loc[pv_system_id]\n",
    "        \n",
    "        # Save data into the Sample dict...\n",
    "        sample['pv_system_id'] = pv_system_id\n",
    "        sample['pv_system_row_number'] = self.pv_metadata.index.get_loc(pv_system_id)\n",
    "        sample['historical_pv_yield'] = selected_pv_power.iloc[:history_len]\n",
    "        sample['target_pv_yield'] = selected_pv_power.iloc[history_len:]\n",
    "        sample['pv_location_x'] = metadata_for_pv_system.location_x\n",
    "        sample['pv_location_y'] = metadata_for_pv_system.location_y\n",
    "        return sample\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NWPDataSource(DataSource):\n",
    "    nwp: xr.Dataset = nwp\n",
    "    params: Iterable[str] = (\n",
    "        't',  # Temperature in Kelvin.\n",
    "        'dswrf',  # Downward short-wave radiation flux in W/m2 (irradiance).\n",
    "        'prate',  # Precipitation rate in kg m^-2 s^-1.\n",
    "        'r',  # Relative humidty in %.\n",
    "        'sde',  # Snow depth in meters.\n",
    "        'si10',  # 10-meter wind speed in m/s.\n",
    "        'vis',  # Visibility in meters.\n",
    "        'lcc',  # Low-level cloud cover in %.\n",
    "        'mcc',  # Medium-level cloud cover in %.\n",
    "        'hcc',  # High-level cloud cover in %.\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __getitem__(self, sample: Sample) -> Sample:\n",
    "        datetime_index = sample['datetime_index'].tz_convert('UTC').tz_convert(None)\n",
    "        start = datetime_index[0]\n",
    "        end = datetime_index[-1]\n",
    "        history_len = sample['history_len']\n",
    "        t0 = datetime_index[history_len]\n",
    "        x = sample['pv_location_x']\n",
    "        y = sample['pv_location_y']\n",
    "        params = list(self.params)\n",
    "        \n",
    "        selected_nwp = get_nwp_example(self.nwp, start=start, end=end, t0=t0)\n",
    "        \n",
    "        # Now select data for nearest to PV system\n",
    "        selected_nwp = selected_nwp.sel(x=x, y=y, method='nearest')[params].to_array()\n",
    "        sample['nwp_above_pv'] = selected_nwp\n",
    "        return sample\n",
    "        \n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class SatelliteDataset(torch.utils.data.IterableDataset):\n",
    "    zarr_chunk_sequences: Iterable[Segment]  #: Defines multiple Zarr chunks to be loaded from disk at once.\n",
    "    history_len: int = 12  #: The number of timesteps of 'history' to load.\n",
    "    forecast_len: int = 1  #: The number of timesteps of 'forecast' to load.\n",
    "        \n",
    "    #: Append additional data, such as PV power or NWPs.\n",
    "    #: Additional data is added before the transforms are applied.\n",
    "    additional_data_sources: Iterable[DataSource] = (PVDataSource(), NWPDataSource())    \n",
    "    transform: Optional[Callable] = None\n",
    "\n",
    "    n_disk_loads_per_epoch: int = 10_000  #: Number of disk loads per worker process per epoch.\n",
    "    min_n_samples_per_disk_load: int = 1_000  #: Number of samples each worker will load for each disk load.\n",
    "    max_n_samples_per_disk_load: int = 2_000  #: Max number of disk loader.  Actual number is chosen randomly between min & max.\n",
    "    n_zarr_chunk_sequences_to_load_at_once: int = 8  #: Number of chunk seqs to load at once.  These are sampled at random.\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        #: Total sequence length of each sample.\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "    def per_worker_init(self, worker_id: int=0) -> None:\n",
    "        \"\"\"Called by worker_init_fn on each copy of SatelliteDataset after the worker process has been spawned.\"\"\"\n",
    "        self.worker_id = worker_id\n",
    "        self.data_array = get_sat_data()\n",
    "        # Each worker must have a different seed for its random number generator.\n",
    "        # Otherwise all the workers will output exactly the same data!\n",
    "        seed = torch.initial_seed()\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "        for data_source in self.additional_data_sources:\n",
    "            data_source.per_worker_init(worker_id)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Asynchronously loads next data from disk while sampling from data_in_mem.\n",
    "        \"\"\"\n",
    "        with futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            future_data = executor.submit(self._load_data_from_disk)\n",
    "            for _ in range(self.n_disk_loads_per_epoch):\n",
    "                data_in_mem = future_data.result()\n",
    "                future_data = executor.submit(self._load_data_from_disk)\n",
    "                n_samples = self.rng.integers(self.min_n_samples_per_disk_load, self.max_n_samples_per_disk_load)\n",
    "                for _ in range(n_samples):\n",
    "                    sample = self._get_sample(data_in_mem)\n",
    "                    \n",
    "                    # Add in additional data.\n",
    "                    for data_source in self.additional_data_sources:\n",
    "                        sample = data_source[sample]\n",
    "\n",
    "                    # Transform.\n",
    "                    if self.transform:\n",
    "                        try:\n",
    "                            sample = self.transform(sample)\n",
    "                        except BadData as e:\n",
    "                            # print(e)\n",
    "                            continue\n",
    "\n",
    "                    yield sample\n",
    "\n",
    "    def _load_data_from_disk(self) -> List[xr.DataArray]:\n",
    "        \"\"\"Loads data from contiguous Zarr chunks from disk into memory.\"\"\"\n",
    "        sat_images_list = []\n",
    "        for _ in range(self.n_zarr_chunk_sequences_to_load_at_once):\n",
    "            zarr_chunk_sequence = self.rng.choice(self.zarr_chunk_sequences)\n",
    "            # rng.choice, weirdly, converts the Segment to a 2-element ndarray.  So let's convert back to\n",
    "            # a Segment and convert to naive datetime:\n",
    "            zarr_chunk_sequence = Segment(*zarr_chunk_sequence)\n",
    "            zarr_chunk_sequence = zarr_chunk_sequence.to_naive()\n",
    "            sat_images = self.data_array.sel(time=slice(*zarr_chunk_sequence))\n",
    "\n",
    "            # Sanity checks\n",
    "            n_timesteps_available = len(sat_images)\n",
    "            if n_timesteps_available < self.total_seq_len:\n",
    "                raise RuntimeError(f'Not enough timesteps in loaded data!  Need at least {self.total_seq_len}.  Got {n_timesteps_available}!')\n",
    "\n",
    "            sat_images_list.append(sat_images.load())\n",
    "        return sat_images_list\n",
    "\n",
    "    def _get_sample(self, sat_data_in_mem_list: List[xr.DataArray]) -> Sample:\n",
    "        # Select a random Zarr chunk sequence from the Zarr chunk sequences loaded into memory\n",
    "        i = self.rng.integers(0, len(sat_data_in_mem_list))\n",
    "        sat_data_in_mem = sat_data_in_mem_list[i]\n",
    "        \n",
    "        # Select random start index\n",
    "        n_timesteps_available = len(sat_data_in_mem)\n",
    "        max_start_idx = n_timesteps_available - self.total_seq_len\n",
    "        start_idx = self.rng.integers(low=0, high=max_start_idx, dtype=np.uint32)\n",
    "        end_idx = start_idx + self.total_seq_len\n",
    "        selected_sat_images = sat_data_in_mem.isel(time=slice(start_idx, end_idx))\n",
    "        datetime_index = pd.DatetimeIndex(selected_sat_images.time.values, tz='UTC') + pd.Timedelta('1 minute')\n",
    "        return Sample(\n",
    "            history_len=self.history_len,\n",
    "            forecast_len=self.forecast_len,\n",
    "            historical_sat_images=selected_sat_images[:self.history_len],\n",
    "            target_sat_images=selected_sat_images[self.history_len:],\n",
    "            datetime_index=datetime_index\n",
    "        )\n",
    "\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"Configures each dataset worker process.\n",
    "    \n",
    "    Just has one job!  To call SatelliteDataset.per_worker_init().\n",
    "    \"\"\"\n",
    "    # get_worker_info() returns information specific to each worker process.\n",
    "    worker_info = torch.utils.data.get_worker_info()\n",
    "    if worker_info is None:\n",
    "        print('worker_info is None!')\n",
    "    else:\n",
    "        dataset_obj = worker_info.dataset  # The Dataset copy in this worker process.\n",
    "        dataset_obj.per_worker_init(worker_id=worker_info.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a910b-30d8-4dc0-b489-f86b8d0e88b4",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab869b-37c2-46df-9249-ac2aa39feff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pv_data_source():\n",
    "    pv_data_source = PVDataSource()\n",
    "    sample = Sample(history_len=1, datetime_index=pd.DatetimeIndex([\"2018-06-01T11:45\", \"2018-06-01T11:50\"], tz='UTC'))\n",
    "    pv_data_source[sample]\n",
    "    return sample\n",
    "\n",
    "sample = test_pv_data_source()\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ffb625-5a74-4eef-8ea0-0ed036d8dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_without_to_tensor = SatelliteDataset(\n",
    "    zarr_chunk_sequences=zarr_chunk_sequences,\n",
    "    transform=Compose([\n",
    "        CropCentredOnPv(),\n",
    "        CheckForBadData(),\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e494ba10-032c-4e29-a467-8f0001822f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_without_to_tensor.per_worker_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b28e4-36a0-4697-9d13-4058bae31bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_iter = dataset_without_to_tensor.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931bfd6-5ed2-4811-a62c-71fa4b9f3b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(dataset_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e5256-8d4b-4d99-ae85-c2a105447e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, figsize=(10, 20))\n",
    "\n",
    "ax = axes[0]\n",
    "sample['historical_sat_images'].isel(time=0).plot(ax=ax)\n",
    "ax.scatter(sample['pv_location_x'], sample['pv_location_y'], c='black')\n",
    "\n",
    "ax = axes[1]\n",
    "sample['target_sat_images'].isel(time=0).plot(ax=ax)\n",
    "ax.scatter(sample['pv_location_x'], sample['pv_location_y'], c='black')\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(pd.concat((sample['historical_pv_yield'], sample['target_pv_yield'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e6407-0434-43bf-a2f9-910e63a2deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['nwp_above_pv'].sel(variable='t').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "dataset = SatelliteDataset(\n",
    "    zarr_chunk_sequences=zarr_chunk_sequences,\n",
    "    transform=Compose([\n",
    "        CropCentredOnPv(),\n",
    "        CheckForBadData(),\n",
    "        ToTensor(),\n",
    "    ]),\n",
    ")\n",
    "\n",
    "if num_workers == 0:\n",
    "    dataset.per_worker_init()\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=num_workers,  # timings:  4=13.8s; 8=11.6; 10=11.3s; 11=11.5s; 12=12.6s.  10=3it/s\n",
    "    worker_init_fn=worker_init_fn,\n",
    "    pin_memory=True,\n",
    "    #multiprocessing_context='spawn'\n",
    "    #persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for batch in dataloader:\n",
    "    print(batch['historical_sat_images'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(batch['datetime_index'].numpy().flatten(), unit='s').values.reshape(-1, 2).astype('datetime64[s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['historical_sat_images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['target_sat_images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['historical_sat_images'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch['historical_sat_images'][0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38072d3-0898-4ace-8581-c4ab8b1c269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch['target_sat_images'][0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aae49c-b0cc-4d2f-8282-5858423a3291",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['historical_pv_yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf388b-e301-4de4-95b9-1da3512fe2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['target_pv_yield']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-northwest",
   "metadata": {},
   "source": [
    "# Simple ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_images_in_model(images, device):    \n",
    "    SAT_IMAGE_MEAN = torch.tensor(93.23458, dtype=torch.float, device=device)\n",
    "    SAT_IMAGE_STD = torch.tensor(115.34247, dtype=torch.float, device=device)\n",
    "    \n",
    "    images = images.float()\n",
    "    images -= SAT_IMAGE_MEAN\n",
    "    images /= SAT_IMAGE_STD\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = 32\n",
    "KERNEL = 3\n",
    "\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_conv1 = nn.Conv2d(in_channels=1, out_channels=CHANNELS//2, kernel_size=KERNEL)\n",
    "        self.encoder_conv2 = nn.Conv2d(in_channels=CHANNELS//2, out_channels=CHANNELS, kernel_size=KERNEL)\n",
    "        self.encoder_conv3 = nn.Conv2d(in_channels=CHANNELS, out_channels=CHANNELS, kernel_size=KERNEL)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=KERNEL)\n",
    "        \n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=CHANNELS * 11 * 11, \n",
    "            out_features=256 - 4  # Minus 4 (1 for the historical PV data; 4 for the PV system embedding)\n",
    "        )\n",
    "        #self.fc_just_prev_yield = nn.Linear(in_features=1, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
    "        self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
    "        self.fc5 = nn.Linear(in_features=128, out_features=1)\n",
    "        \n",
    "        self.pv_system_id_embedding = nn.Embedding(\n",
    "            num_embeddings=len(pv_metadata),\n",
    "            embedding_dim=4\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        images = x['target_sat_images']\n",
    "        images = normalise_images_in_model(images, self.device)\n",
    "        \n",
    "        # Pass data through the network :)\n",
    "        out = F.relu(self.encoder_conv1(images))\n",
    "        out = self.maxpool(out)\n",
    "        out = F.relu(self.encoder_conv2(out))\n",
    "        out = self.maxpool(out)\n",
    "        out = F.relu(self.encoder_conv3(out))\n",
    "        \n",
    "        out = out.view(-1, CHANNELS * 11 * 11)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        \n",
    "        pv_embedding = self.pv_system_id_embedding(x['pv_system_row_number'])\n",
    "        \n",
    "        out = torch.cat(\n",
    "            (\n",
    "                out, \n",
    "                #x['historical_pv_yield'][:, :1], \n",
    "                pv_embedding\n",
    "            ), dim=1)\n",
    "\n",
    "        #out = F.relu(self.fc_just_prev_yield(x['historical_pv_yield'][:, :1]))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.relu(self.fc3(out))\n",
    "        out = F.relu(self.fc4(out))\n",
    "        out = self.fc5(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def _training_or_validation_step(self, batch, is_train_step):\n",
    "        y_hat = self(batch)\n",
    "        y = batch['target_pv_yield']\n",
    "        #mse_loss = F.mse_loss(y_hat, y)\n",
    "        mae_loss = (y_hat - y).abs().mean()\n",
    "        tag = \"Train\" if is_train_step else \"Validation\"\n",
    "        #self.log_dict({'MSE/' + tag: mse_loss}, on_step=is_train_step, on_epoch=True)\n",
    "        self.log_dict({'MAE/' + tag: mae_loss}, on_step=is_train_step, on_epoch=True)\n",
    "        return mae_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._training_or_validation_step(batch, is_train_step=True)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._training_or_validation_step(batch, is_train_step=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitAutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b528596-61e5-415b-9bec-83c2f5e2e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=400, terminate_on_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.fit(model, train_dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb610f0-9c2c-4b47-94d3-899a5dd5337e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_pv_yield",
   "language": "python",
   "name": "predict_pv_yield"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
