{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, using standard PyTorch; then using Lighting:\n",
    "\n",
    "1. Using dummy (random) data, load, say, 8GB into pinned system RAM.\n",
    "2. Push into GPU memory at start of training loop.\n",
    "3. Train batch\n",
    "\n",
    "Time each step.  See how to make steps async.\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "4 GBytes copy from CPU memory to GPU memory:\n",
    "\n",
    "```\n",
    "  Unpinned: 3.6 s\n",
    "    Pinned: 0.3 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.34 s, sys: 1.67 s, total: 11 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tensor_cpu_unpinned = torch.randn(1000, 1000, 1000)  # 4 GBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.51 s, sys: 1.5 s, total: 4.01 s\n",
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tensor_cuda_from_unpinned = tensor_cpu_unpinned.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.22 s, sys: 2.89 s, total: 5.1 s\n",
      "Wall time: 4.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tensor_pinned = tensor_cpu_unpinned.pin_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.57 ms, sys: 4.93 ms, total: 7.49 ms\n",
      "Wall time: 6.24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tensor_cuda_from_pinned = tensor_pinned.cuda(non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a contiguous chunk keeps the pinned memory.\n",
    "tensor_pinned[:10].is_pinned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking randome chunk stops the pinning.\n",
    "tensor_pinned[[1, 5, 7, 10]].is_pinned()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next up:  Put this into a simple DataLoader; and then in a Lightning training loop (where the forward and train functions do nothing, so the only delay is loading memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic plan:\n",
    "\n",
    "* Dataset loads a \"big chunk\" of data into (unpinned) CPU memory.\n",
    "  - When we do this for real, this could be done in a separate process.\n",
    "* Dataset then yields batches (sampled from the \"big chunk\" in memory).\n",
    "* All DataLoader does is turn the numpy arrays into pinned tensors.  It doesn't do any sampling.  It gets a pre-made batch from Dataset.\n",
    "* The training loop in Lightning then asynchonously loads that pinned data into GPU memory while the GPU is training.  The batch needs to be pinned to enable async copies into GPU RAM.\n",
    "\n",
    "Things that won't work:\n",
    "\n",
    "* I don't think we can use multiple processes to load data into the GPU or into pinned memory.  How would we share that memory between processes?\n",
    "* Pinning the \"big chunk\" probably isn't useful, because the sampled data isn't pinned **if we take random samples**.  But taking a contiguous chunk of pinned memory retains the pinning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, batch_size=64, n_loads_from_disk=2, n_samples_from_mem=4):\n",
    "        self.rng = np.random.default_rng()\n",
    "        self.batch_size = batch_size\n",
    "        self.n_loads_from_disk = n_loads_from_disk\n",
    "        self.n_samples_from_mem = n_samples_from_mem\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for _ in range(self.n_loads_from_disk):\n",
    "            data = self.rng.random(size=(1000, 256, 256), dtype=np.float32)\n",
    "            #n = len(data)\n",
    "            # data = torch.tensor(data).pin_memory()\n",
    "            for i in range(self.n_samples_from_mem):\n",
    "                #idx = torch.randint(high=n, size=(self.batch_size, ))\n",
    "                #yield data[idx]\n",
    "                yield self.rng.choice(data, self.batch_size)\n",
    "                #yield tensor[i*self.batch_size:(i+1)*self.batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyIterableDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 65 µs, sys: 11 µs, total: 76 µs\n",
      "Wall time: 78.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    #collate_fn=lambda x: x[0],\n",
    "    #collate_fn=lambda x: torch.tensor(x[0], device='cuda'),\n",
    "    collate_fn=lambda x: torch.tensor(x[0]),\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 256])\n",
      "True\n",
      "torch.Size([64, 256, 256])\n",
      "True\n",
      "torch.Size([64, 256, 256])\n",
      "True\n",
      "torch.Size([64, 256, 256])\n",
      "True\n",
      "torch.Size([64, 256, 256])\n",
      "True\n",
      "torch.Size([64, 256, 256])\n",
      "True\n",
      "torch.Size([64, 256, 256])\n",
      "True\n",
      "torch.Size([64, 256, 256])\n",
      "True\n",
      "CPU times: user 1.14 s, sys: 103 ms, total: 1.24 s\n",
      "Wall time: 531 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for d in dataloader:\n",
    "    print(d.shape)\n",
    "    print(d.is_pinned())\n",
    "    #d.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* copying numpy array to CUDA Tensor within DataLoader = 8.48s (but this won't allow us to async load into GPU)\n",
    "* Pinning batch by batch (pinning done by DataLoader) = 8.63s\n",
    "* No pinning = 8.71s\n",
    "* Pinning the \"big chunk\", and then taking contiguous slices: 9.64s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.5'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(256 * 256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256 * 256)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitAutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 4 M   \n",
      "1 | decoder | Sequential | 4 M   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/miniconda3/envs/predict_pv_yield/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: : 8it [00:00,  8.33it/s, loss=0.083, v_num=5]\n",
      "CPU times: user 1min 4s, sys: 11.3 s, total: 1min 15s\n",
      "Wall time: 31.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_pv_yield_2",
   "language": "python",
   "name": "predict_pv_yield_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
